{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Loading - The Load Component\n",
    "\n",
    "### Scenario\n",
    "You've Extracted data from APIs and Transformed it with business logic. Now it's time to Load it into your data warehouse.\n",
    "\n",
    "### Business Context\n",
    "Your customer service team needs this enriched data in their CRM system for better customer support.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Database Setup and Connection\n",
    "\n",
    "Connect to SQL Server and create the customer data warehouse database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "SERVER = 'localhost'  # Your SQL Server instance\n",
    "DATABASE = 'customer_warehouse'  # We'll create this database\n",
    "\n",
    "# Connection string for initial setup (master database)\n",
    "master_connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SERVER};DATABASE=master;Trusted_Connection=yes;'\n",
    "\n",
    "print(\"=== DATABASE SETUP ===\")\n",
    "try:\n",
    "    # Connect to master database to create our warehouse (with autocommit for CREATE DATABASE)\n",
    "    master_conn = pyodbc.connect(master_connection_string, autocommit=True)\n",
    "    master_cursor = master_conn.cursor()\n",
    "    \n",
    "    # Create database if it doesn't exist\n",
    "    try:\n",
    "        master_cursor.execute(f\"CREATE DATABASE {DATABASE}\")\n",
    "        print(f\"‚úÖ Created database: {DATABASE}\")\n",
    "    except pyodbc.Error as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            print(f\"‚ÑπÔ∏è  Database {DATABASE} already exists\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Database creation issue: {e}\")\n",
    "    \n",
    "    master_conn.close()\n",
    "    print(\"Database setup complete\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Database setup failed: {e}\")\n",
    "    print(\"Please check your SQL Server connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Data Warehouse Tables\n",
    "\n",
    "Create tables optimized for analytical queries and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to our customer warehouse database\n",
    "warehouse_connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};Trusted_Connection=yes;'\n",
    "\n",
    "def create_warehouse_tables():\n",
    "    \"\"\"Create optimized data warehouse tables\"\"\"\n",
    "    \n",
    "    # Table creation scripts\n",
    "    create_tables_sql = \"\"\"\n",
    "    -- Drop existing tables if they exist\n",
    "    IF OBJECT_ID('dbo.customer_enriched', 'U') IS NOT NULL DROP TABLE dbo.customer_enriched;\n",
    "    IF OBJECT_ID('dbo.enrichment_audit', 'U') IS NOT NULL DROP TABLE dbo.enrichment_audit;\n",
    "    \n",
    "    -- Main customer data table\n",
    "    CREATE TABLE customer_enriched (\n",
    "        customer_id INT PRIMARY KEY,\n",
    "        first_name NVARCHAR(50) NOT NULL,\n",
    "        last_name NVARCHAR(50) NOT NULL,\n",
    "        email NVARCHAR(100) NOT NULL,\n",
    "        phone NVARCHAR(20),\n",
    "        postcode NVARCHAR(10),\n",
    "        \n",
    "        -- Geographic enrichment\n",
    "        region NVARCHAR(50),\n",
    "        country NVARCHAR(50),\n",
    "        district NVARCHAR(50),\n",
    "        longitude DECIMAL(10,7),\n",
    "        latitude DECIMAL(10,7),\n",
    "        geo_enriched BIT DEFAULT 0,\n",
    "        \n",
    "        -- Business enrichment\n",
    "        company NVARCHAR(100),\n",
    "        company_size NVARCHAR(50),\n",
    "        industry NVARCHAR(50),\n",
    "        annual_revenue NVARCHAR(50),\n",
    "        is_business BIT DEFAULT 0,\n",
    "        \n",
    "        -- Risk assessment\n",
    "        calculated_risk NVARCHAR(20),\n",
    "        risk_score_numeric INT,\n",
    "        risk_factors NVARCHAR(500),\n",
    "        \n",
    "        -- Account status\n",
    "        status NVARCHAR(20),\n",
    "        \n",
    "        -- ETL metadata\n",
    "        processed_date DATETIME2 DEFAULT GETDATE(),\n",
    "        data_source NVARCHAR(50),\n",
    "        enrichment_status NVARCHAR(50),\n",
    "        \n",
    "        -- Audit fields\n",
    "        created_date DATETIME2 DEFAULT GETDATE(),\n",
    "        modified_date DATETIME2 DEFAULT GETDATE()\n",
    "    );\n",
    "    \n",
    "    -- Audit table for tracking all loading operations\n",
    "    CREATE TABLE enrichment_audit (\n",
    "        audit_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "        batch_id UNIQUEIDENTIFIER DEFAULT NEWID(),\n",
    "        operation_type NVARCHAR(20), -- INSERT, UPDATE, DELETE\n",
    "        records_processed INT,\n",
    "        records_successful INT,\n",
    "        records_failed INT,\n",
    "        processing_start DATETIME2,\n",
    "        processing_end DATETIME2,\n",
    "        duration_seconds AS DATEDIFF(SECOND, processing_start, processing_end),\n",
    "        error_message NVARCHAR(1000),\n",
    "        pipeline_version NVARCHAR(20)\n",
    "    );\n",
    "    \n",
    "    -- Create indexes for better query performance\n",
    "    CREATE INDEX IX_customer_enriched_region ON customer_enriched(region);\n",
    "    CREATE INDEX IX_customer_enriched_risk ON customer_enriched(calculated_risk);\n",
    "    CREATE INDEX IX_customer_enriched_business ON customer_enriched(is_business);\n",
    "    CREATE INDEX IX_customer_enriched_status ON customer_enriched(status);\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        conn = pyodbc.connect(warehouse_connection_string)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute table creation\n",
    "        cursor.execute(create_tables_sql)\n",
    "        conn.commit()\n",
    "        \n",
    "        print(\"‚úÖ Data warehouse tables created successfully\")\n",
    "        print(\"   - customer_enriched (main data table)\")\n",
    "        print(\"   - enrichment_audit (processing audit trail)\")\n",
    "        print(\"   - Performance indexes created\")\n",
    "        \n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Table creation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create the tables\n",
    "print(\"=== CREATING DATA WAREHOUSE TABLES ===\")\n",
    "table_creation_success = create_warehouse_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Sample Enriched Data\n",
    "\n",
    "For this proof-ofconcept, create sample enriched data\n",
    "\n",
    "**#TODO** Extract this data from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample enriched customer data (from our API enrichment lab)\n",
    "enriched_customers = {\n",
    "    'customer_id': [1001, 1002, 1003, 1004, 1005, 1006],\n",
    "    'first_name': ['John', 'Jane', 'Mike', 'Sarah', 'Bob', 'Alice'],\n",
    "    'last_name': ['Smith', 'Doe', 'Johnson', 'Wilson', 'Brown', 'Cooper'],\n",
    "    'email': ['john@email.com', 'jane@email.com', 'mike@techcorp.com', \n",
    "              'sarah@retailplus.com', 'bob@email.com', 'alice@freelance.com'],\n",
    "    'phone': ['01234567890', '01987654321', '01555123456', \n",
    "              '01777888999', '01111222333', '01444555666'],\n",
    "    'postcode': ['SW1A 1AA', 'M1 1AF', 'B1 1BB', 'LS1 2AJ', 'NE1 3NG', 'CF10 2HH'],\n",
    "    \n",
    "    # Geographic enrichment\n",
    "    'region': ['London', 'North West', 'West Midlands', 'Yorkshire and The Humber', 'North East', 'Wales'],\n",
    "    'country': ['England', 'England', 'England', 'England', 'England', 'Wales'],\n",
    "    'district': ['Westminster', 'Manchester', 'Birmingham', 'Leeds', 'Newcastle', 'Cardiff'],\n",
    "    'longitude': [-0.1419, -2.2426, -1.8904, -1.5491, -1.6131, -3.1791],\n",
    "    'latitude': [51.5014, 53.4794, 52.4796, 53.7997, 54.9738, 51.4816],\n",
    "    'geo_enriched': [1, 1, 1, 1, 1, 1],  # Changed from True to 1 for SQL BIT compatibility\n",
    "    \n",
    "    # Business enrichment\n",
    "    'company': ['', '', 'TechCorp Ltd', 'Retail Plus', '', 'Freelance Design'],\n",
    "    'company_size': ['Individual', 'Individual', 'Medium (50-250 employees)', 'Large (250+ employees)', 'Individual', 'Micro (1-10 employees)'],\n",
    "    'industry': ['Personal', 'Personal', 'Technology', 'Retail', 'Personal', 'Creative Services'],\n",
    "    'annual_revenue': ['N/A', 'N/A', '¬£2M-¬£10M', '¬£10M+', 'N/A', '¬£0-¬£100K'],\n",
    "    'is_business': [0, 0, 1, 1, 0, 1],  # Changed from False/True to 0/1 for SQL BIT compatibility\n",
    "    \n",
    "    # Risk assessment\n",
    "    'calculated_risk': ['Low', 'Low', 'Medium', 'High', 'Low', 'Medium'],\n",
    "    'risk_score_numeric': [0, 0, 2, 5, 0, 1],\n",
    "    'risk_factors': ['Standard profile', 'Standard profile', 'High-risk region', \n",
    "                    'Account suspended; High-risk region', 'Standard profile', 'Small business'],\n",
    "    \n",
    "    # Account status\n",
    "    'status': ['active', 'active', 'active', 'suspended', 'active', 'active'],\n",
    "    \n",
    "    # ETL metadata\n",
    "    'processed_date': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')] * 6,\n",
    "    'data_source': ['ETL_Pipeline_v1'] * 6,\n",
    "    'enrichment_status': ['Fully Enriched', 'Fully Enriched', 'Fully Enriched', \n",
    "                         'Fully Enriched', 'Fully Enriched', 'Fully Enriched']\n",
    "}\n",
    "\n",
    "df_enriched = pd.DataFrame(enriched_customers)\n",
    "\n",
    "print(\"=== ENRICHED CUSTOMER DATA TO LOAD ===\")\n",
    "print(f\"Records to load: {len(df_enriched)}\")\n",
    "print(\"\\nSample records:\")\n",
    "display_cols = ['customer_id', 'first_name', 'last_name', 'region', 'industry', 'calculated_risk']\n",
    "print(df_enriched[display_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Robust Database Loading\n",
    "\n",
    "Create a production-ready loading process with error handling, audit logging, and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "class DatabaseLoader:\n",
    "    \"\"\"Production database loading with comprehensive error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, connection_string: str):\n",
    "        self.connection_string = connection_string\n",
    "        self.batch_id = str(uuid.uuid4())\n",
    "        \n",
    "    def load_enriched_customers(self, df_customers: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Load enriched customer data with comprehensive error handling\n",
    "        Returns: Loading statistics and results\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        results = {\n",
    "            'batch_id': self.batch_id,\n",
    "            'total_records': len(df_customers),\n",
    "            'successful_inserts': 0,\n",
    "            'successful_updates': 0,\n",
    "            'failed_records': 0,\n",
    "            'errors': [],\n",
    "            'processing_time': 0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            conn = pyodbc.connect(self.connection_string)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            print(f\"üîÑ Starting batch load: {self.batch_id}\")\n",
    "            print(f\"üìä Processing {len(df_customers)} customer records\")\n",
    "            \n",
    "            # Process each customer with upsert logic\n",
    "            for index, customer in df_customers.iterrows():\n",
    "                try:\n",
    "                    # Check if customer already exists\n",
    "                    check_sql = \"SELECT COUNT(*) FROM customer_enriched WHERE customer_id = ?\"\n",
    "                    cursor.execute(check_sql, customer['customer_id'])\n",
    "                    exists = cursor.fetchone()[0] > 0\n",
    "                    \n",
    "                    if exists:\n",
    "                        # UPDATE existing record\n",
    "                        update_sql = \"\"\"\n",
    "                        UPDATE customer_enriched SET\n",
    "                            first_name = ?, last_name = ?, email = ?, phone = ?, postcode = ?,\n",
    "                            region = ?, country = ?, district = ?, longitude = ?, latitude = ?, geo_enriched = ?,\n",
    "                            company = ?, company_size = ?, industry = ?, annual_revenue = ?, is_business = ?,\n",
    "                            calculated_risk = ?, risk_score_numeric = ?, risk_factors = ?,\n",
    "                            status = ?, processed_date = ?, data_source = ?, enrichment_status = ?,\n",
    "                            modified_date = GETDATE()\n",
    "                        WHERE customer_id = ?\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        cursor.execute(update_sql, (\n",
    "                            customer['first_name'], customer['last_name'], customer['email'],\n",
    "                            customer['phone'], customer['postcode'],\n",
    "                            customer['region'], customer['country'], customer['district'],\n",
    "                            customer['longitude'], customer['latitude'], customer['geo_enriched'],\n",
    "                            customer['company'], customer['company_size'], customer['industry'],\n",
    "                            customer['annual_revenue'], customer['is_business'],\n",
    "                            customer['calculated_risk'], customer['risk_score_numeric'], customer['risk_factors'],\n",
    "                            customer['status'], customer['processed_date'], customer['data_source'],\n",
    "                            customer['enrichment_status'], customer['customer_id']\n",
    "                        ))\n",
    "                        \n",
    "                        results['successful_updates'] += 1\n",
    "                        print(f\"  ‚úèÔ∏è  Updated customer {customer['customer_id']}\")\n",
    "                        \n",
    "                    else:\n",
    "                        # INSERT new record\n",
    "                        insert_sql = \"\"\"\n",
    "                        INSERT INTO customer_enriched (\n",
    "                            customer_id, first_name, last_name, email, phone, postcode,\n",
    "                            region, country, district, longitude, latitude, geo_enriched,\n",
    "                            company, company_size, industry, annual_revenue, is_business,\n",
    "                            calculated_risk, risk_score_numeric, risk_factors,\n",
    "                            status, processed_date, data_source, enrichment_status\n",
    "                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        cursor.execute(insert_sql, (\n",
    "                            customer['customer_id'], customer['first_name'], customer['last_name'],\n",
    "                            customer['email'], customer['phone'], customer['postcode'],\n",
    "                            customer['region'], customer['country'], customer['district'],\n",
    "                            customer['longitude'], customer['latitude'], customer['geo_enriched'],\n",
    "                            customer['company'], customer['company_size'], customer['industry'],\n",
    "                            customer['annual_revenue'], customer['is_business'],\n",
    "                            customer['calculated_risk'], customer['risk_score_numeric'], customer['risk_factors'],\n",
    "                            customer['status'], customer['processed_date'], customer['data_source'],\n",
    "                            customer['enrichment_status']\n",
    "                        ))\n",
    "                        \n",
    "                        results['successful_inserts'] += 1\n",
    "                        print(f\"  ‚ûï Inserted customer {customer['customer_id']}\")\n",
    "                \n",
    "                except Exception as record_error:\n",
    "                    error_msg = f\"Customer {customer['customer_id']}: {str(record_error)}\"\n",
    "                    results['errors'].append(error_msg)\n",
    "                    results['failed_records'] += 1\n",
    "                    print(f\"  ‚ùå Failed: {error_msg}\")\n",
    "                    \n",
    "            # Commit all changes\n",
    "            conn.commit()\n",
    "            \n",
    "            # Record processing time\n",
    "            end_time = datetime.now()\n",
    "            results['processing_time'] = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            # Log audit information\n",
    "            self._log_audit_record(cursor, start_time, end_time, results)\n",
    "            conn.commit()\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            print(f\"\\n‚úÖ Batch load completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['errors'].append(f\"Critical loading error: {str(e)}\")\n",
    "            print(f\"‚ùå Critical loading error: {e}\")\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _log_audit_record(self, cursor, start_time, end_time, results):\n",
    "        \"\"\"Log detailed audit information\"\"\"\n",
    "        audit_sql = \"\"\"\n",
    "        INSERT INTO enrichment_audit (\n",
    "            batch_id, operation_type, records_processed, records_successful, \n",
    "            records_failed, processing_start, processing_end, error_message, pipeline_version\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        \n",
    "        error_summary = '; '.join(results['errors'][:5]) if results['errors'] else None\n",
    "        successful_records = results['successful_inserts'] + results['successful_updates']\n",
    "        \n",
    "        cursor.execute(audit_sql, (\n",
    "            self.batch_id,\n",
    "            'UPSERT',\n",
    "            results['total_records'],\n",
    "            successful_records,\n",
    "            results['failed_records'],\n",
    "            start_time,\n",
    "            end_time,\n",
    "            error_summary,\n",
    "            'ETL_Pipeline_v1.0'\n",
    "        ))\n",
    "\n",
    "# Execute the loading process\n",
    "print(\"=== LOADING ENRICHED DATA TO DATABASE ===\")\n",
    "\n",
    "if table_creation_success:\n",
    "    loader = DatabaseLoader(warehouse_connection_string)\n",
    "    loading_results = loader.load_enriched_customers(df_enriched)\n",
    "    \n",
    "    print(\"\\n=== LOADING RESULTS ===\")\n",
    "    print(f\"Batch ID: {loading_results['batch_id']}\")\n",
    "    print(f\"Total records: {loading_results['total_records']}\")\n",
    "    print(f\"Successful inserts: {loading_results['successful_inserts']}\")\n",
    "    print(f\"Successful updates: {loading_results['successful_updates']}\")\n",
    "    print(f\"Failed records: {loading_results['failed_records']}\")\n",
    "    print(f\"Processing time: {loading_results['processing_time']:.2f} seconds\")\n",
    "    \n",
    "    if loading_results['errors']:\n",
    "        print(\"\\nErrors encountered:\")\n",
    "        for error in loading_results['errors']:\n",
    "            print(f\"  ‚ö†Ô∏è  {error}\")\n",
    "else:\n",
    "    print(\"‚ùå Skipping data loading due to table creation failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Validation and Quality Checks\n",
    "\n",
    "Verify that the data was loaded correctly and perform quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_loaded_data():\n",
    "    \"\"\"Comprehensive validation of loaded data\"\"\"\n",
    "    \n",
    "    # Set up SQLAlchemy engine for pandas compatibility\n",
    "    params = urllib.parse.quote_plus(warehouse_connection_string)\n",
    "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"=== DATA VALIDATION CHECKS ===\")\n",
    "        \n",
    "        # 1. Record count validation\n",
    "        count_query = \"SELECT COUNT(*) FROM customer_enriched\"\n",
    "        total_records = pd.read_sql(count_query, engine).iloc[0, 0]\n",
    "        print(f\"‚úÖ Total records in database: {total_records}\")\n",
    "        \n",
    "        # 2. Data completeness checks\n",
    "        completeness_query = \"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            SUM(CASE WHEN first_name IS NOT NULL AND first_name != '' THEN 1 ELSE 0 END) as complete_names,\n",
    "            SUM(CASE WHEN email IS NOT NULL AND email != '' THEN 1 ELSE 0 END) as complete_emails,\n",
    "            SUM(CASE WHEN geo_enriched = 1 THEN 1 ELSE 0 END) as geo_enriched_count,\n",
    "            SUM(CASE WHEN is_business = 1 THEN 1 ELSE 0 END) as business_customers\n",
    "        FROM customer_enriched\n",
    "        \"\"\"\n",
    "        \n",
    "        completeness_df = pd.read_sql(completeness_query, engine)\n",
    "        comp = completeness_df.iloc[0]\n",
    "        \n",
    "        print(f\"‚úÖ Name completeness: {comp['complete_names']}/{comp['total_records']} ({comp['complete_names']/comp['total_records']:.1%})\")\n",
    "        print(f\"‚úÖ Email completeness: {comp['complete_emails']}/{comp['total_records']} ({comp['complete_emails']/comp['total_records']:.1%})\")\n",
    "        print(f\"‚úÖ Geographic enrichment: {comp['geo_enriched_count']}/{comp['total_records']} ({comp['geo_enriched_count']/comp['total_records']:.1%})\")\n",
    "        print(f\"‚úÖ Business customers: {comp['business_customers']}/{comp['total_records']} ({comp['business_customers']/comp['total_records']:.1%})\")\n",
    "        \n",
    "        # 3. Risk distribution analysis\n",
    "        risk_query = \"\"\"\n",
    "        SELECT \n",
    "            calculated_risk,\n",
    "            COUNT(*) as customer_count,\n",
    "            CAST(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER() AS DECIMAL(5,1)) as percentage\n",
    "        FROM customer_enriched \n",
    "        GROUP BY calculated_risk\n",
    "        ORDER BY customer_count DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        risk_df = pd.read_sql(risk_query, engine)\n",
    "        print(f\"\\n‚úÖ Risk Distribution:\")\n",
    "        for _, row in risk_df.iterrows():\n",
    "            print(f\"   {row['calculated_risk']} Risk: {row['customer_count']} customers ({row['percentage']}%)\")\n",
    "        \n",
    "        # 4. Geographic distribution\n",
    "        geo_query = \"\"\"\n",
    "        SELECT TOP 5\n",
    "            region,\n",
    "            COUNT(*) as customer_count\n",
    "        FROM customer_enriched \n",
    "        WHERE region IS NOT NULL AND region != 'Unknown'\n",
    "        GROUP BY region\n",
    "        ORDER BY customer_count DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        geo_df = pd.read_sql(geo_query, engine)\n",
    "        print(f\"\\n‚úÖ Top Regions by Customer Count:\")\n",
    "        for _, row in geo_df.iterrows():\n",
    "            print(f\"   {row['region']}: {row['customer_count']} customers\")\n",
    "        \n",
    "        # 5. Audit trail verification\n",
    "        audit_query = \"\"\"\n",
    "        SELECT \n",
    "            batch_id,\n",
    "            operation_type,\n",
    "            records_processed,\n",
    "            records_successful,\n",
    "            records_failed,\n",
    "            duration_seconds,\n",
    "            processing_start\n",
    "        FROM enrichment_audit \n",
    "        ORDER BY processing_start DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        audit_df = pd.read_sql(audit_query, engine)\n",
    "        print(f\"\\n‚úÖ Recent Processing Batches:\")\n",
    "        for _, row in audit_df.iterrows():\n",
    "            success_rate = (row['records_successful'] / row['records_processed'] * 100) if row['records_processed'] > 0 else 0\n",
    "            print(f\"   Batch: {str(row['batch_id'])[:8]}... | {row['records_processed']} records | {success_rate:.1f}% success | {row['duration_seconds']}s\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run validation\n",
    "validation_success = validate_loaded_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Business Intelligence Queries\n",
    "\n",
    "Demonstrate the value of enriched data with business-relevant queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_business_insights():\n",
    "    \"\"\"Generate business insights from enriched customer data\"\"\"\n",
    "    \n",
    "    # Set up SQLAlchemy engine for pandas compatibility\n",
    "    params = urllib.parse.quote_plus(warehouse_connection_string)\n",
    "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"=== BUSINESS INSIGHTS FROM ENRICHED DATA ===\")\n",
    "        \n",
    "        # 1. High-value business customers by region\n",
    "        high_value_query = \"\"\"\n",
    "        SELECT \n",
    "            region,\n",
    "            COUNT(*) as business_customers,\n",
    "            SUM(CASE WHEN calculated_risk = 'Low' THEN 1 ELSE 0 END) as low_risk_businesses\n",
    "        FROM customer_enriched \n",
    "        WHERE is_business = 1\n",
    "        GROUP BY region\n",
    "        ORDER BY business_customers DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        high_value_df = pd.read_sql(high_value_query, engine)\n",
    "        print(\"\\nüìä Business Customers by Region:\")\n",
    "        for _, row in high_value_df.iterrows():\n",
    "            low_risk_pct = (row['low_risk_businesses'] / row['business_customers'] * 100) if row['business_customers'] > 0 else 0\n",
    "            print(f\"   {row['region']}: {row['business_customers']} businesses ({row['low_risk_businesses']} low-risk, {low_risk_pct:.0f}%)\")\n",
    "        \n",
    "        # 2. Risk assessment for customer support prioritization\n",
    "        support_priority_query = \"\"\"\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            first_name + ' ' + last_name as customer_name,\n",
    "            company,\n",
    "            region,\n",
    "            calculated_risk,\n",
    "            risk_factors,\n",
    "            status\n",
    "        FROM customer_enriched \n",
    "        WHERE calculated_risk IN ('High', 'Medium')\n",
    "        ORDER BY \n",
    "            CASE calculated_risk WHEN 'High' THEN 1 WHEN 'Medium' THEN 2 ELSE 3 END,\n",
    "            customer_name\n",
    "        \"\"\"\n",
    "        \n",
    "        priority_df = pd.read_sql(support_priority_query, engine)\n",
    "        print(f\"\\nüö® Customer Support Priority List ({len(priority_df)} customers):\")\n",
    "        for _, row in priority_df.iterrows():\n",
    "            risk_icon = \"üî¥\" if row['calculated_risk'] == 'High' else \"üü°\"\n",
    "            company_info = f\" ({row['company']})\" if row['company'] else \"\"\n",
    "            print(f\"   {risk_icon} {row['customer_name']}{company_info} - {row['region']} - {row['risk_factors']}\")\n",
    "        \n",
    "        # 3. Geographic expansion opportunities\n",
    "        expansion_query = \"\"\"\n",
    "        SELECT \n",
    "            region,\n",
    "            COUNT(*) as total_customers,\n",
    "            SUM(CASE WHEN is_business = 1 THEN 1 ELSE 0 END) as business_customers,\n",
    "            SUM(CASE WHEN status = 'active' THEN 1 ELSE 0 END) as active_customers,\n",
    "            AVG(CAST(risk_score_numeric AS FLOAT)) as avg_risk_score\n",
    "        FROM customer_enriched \n",
    "        GROUP BY region\n",
    "        ORDER BY total_customers DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        expansion_df = pd.read_sql(expansion_query, engine)\n",
    "        print(f\"\\nüéØ Market Analysis by Region:\")\n",
    "        for _, row in expansion_df.iterrows():\n",
    "            business_pct = (row['business_customers'] / row['total_customers'] * 100) if row['total_customers'] > 0 else 0\n",
    "            active_pct = (row['active_customers'] / row['total_customers'] * 100) if row['total_customers'] > 0 else 0\n",
    "            print(f\"   {row['region']}: {row['total_customers']} customers | {business_pct:.0f}% business | {active_pct:.0f}% active | Risk: {row['avg_risk_score']:.1f}\")\n",
    "        \n",
    "        # 4. Data quality scorecard\n",
    "        quality_query = \"\"\"\n",
    "        SELECT \n",
    "            enrichment_status,\n",
    "            COUNT(*) as customer_count,\n",
    "            AVG(CASE WHEN geo_enriched = 1 THEN 100.0 ELSE 0.0 END) as geo_completion_rate,\n",
    "            AVG(CASE WHEN is_business = 1 AND company IS NOT NULL AND company != '' THEN 100.0 \n",
    "                     WHEN is_business = 0 THEN 100.0 ELSE 0.0 END) as business_data_quality\n",
    "        FROM customer_enriched \n",
    "        GROUP BY enrichment_status\n",
    "        ORDER BY customer_count DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        quality_df = pd.read_sql(quality_query, engine)\n",
    "        print(f\"\\nüìà Data Quality Scorecard:\")\n",
    "        for _, row in quality_df.iterrows():\n",
    "            print(f\"   {row['enrichment_status']}: {row['customer_count']} customers | Geo: {row['geo_completion_rate']:.0f}% | Business: {row['business_data_quality']:.0f}%\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Business insights generation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Generate insights\n",
    "if validation_success:\n",
    "    insights_success = generate_business_insights()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping business insights due to validation issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Upsert Functionality\n",
    "\n",
    "Demonstrate how the pipeline handles updates to existing customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test upsert functionality with updated customer data\n",
    "def test_upsert_functionality():\n",
    "    \"\"\"Test how the pipeline handles existing customer updates\"\"\"\n",
    "    \n",
    "    print(\"=== TESTING UPSERT FUNCTIONALITY ===\")\n",
    "    \n",
    "    # Create updated customer data (some existing, some new)\n",
    "    updated_customers = {\n",
    "        'customer_id': [1001, 1002, 1007, 1008],  # 1001,1002 exist, 1007,1008 are new\n",
    "        'first_name': ['John', 'Jane', 'David', 'Emma'],\n",
    "        'last_name': ['Smith', 'Doe', 'Taylor', 'Watson'],\n",
    "        'email': ['john.smith@newemail.com', 'jane@email.com', 'david@company.com', 'emma@startup.com'],\n",
    "        'phone': ['01234567890', '01987654321', '01666777888', '01999888777'],\n",
    "        'postcode': ['SW1A 1AA', 'M1 1AH', 'E1 0AD', 'EC1A 1BB'],\n",
    "        \n",
    "        # Geographic enrichment\n",
    "        'region': ['London', 'North West', 'London', 'London'],\n",
    "        'country': ['England', 'England', 'England', 'England'],\n",
    "        'district': ['Westminster', 'Manchester', 'Tower Hamlets', 'City of London'],\n",
    "        'longitude': [-0.1419, -2.2426, -0.0713, -0.0982],\n",
    "        'latitude': [51.5014, 53.4794, 51.5206, 51.5155],\n",
    "        'geo_enriched': [1, 1, 1, 1],  # Changed from True to 1\n",
    "        \n",
    "        # Business enrichment\n",
    "        'company': ['', '', 'Tech Solutions Ltd', 'Innovation Startup'],\n",
    "        'company_size': ['Individual', 'Individual', 'Small (10-50 employees)', 'Micro (1-10 employees)'],\n",
    "        'industry': ['Personal', 'Personal', 'Technology', 'Technology'],\n",
    "        'annual_revenue': ['N/A', 'N/A', '¬£100K-¬£2M', '¬£0-¬£100K'],\n",
    "        'is_business': [0, 0, 1, 1],  # Changed from False/True to 0/1\n",
    "        \n",
    "        # Risk assessment\n",
    "        'calculated_risk': ['Low', 'Low', 'Medium', 'High'],\n",
    "        'risk_score_numeric': [0, 0, 2, 3],\n",
    "        'risk_factors': ['Standard profile', 'Standard profile', 'High-risk region', \n",
    "                        'New business; High-risk region'],\n",
    "        \n",
    "        # Account status\n",
    "        'status': ['active', 'active', 'active', 'active'],\n",
    "        \n",
    "        # ETL metadata\n",
    "        'processed_date': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')] * 4,\n",
    "        'data_source': ['ETL_Pipeline_v1_Update'] * 4,\n",
    "        'enrichment_status': ['Fully Enriched'] * 4\n",
    "    }\n",
    "    \n",
    "    df_updates = pd.DataFrame(updated_customers)\n",
    "    \n",
    "    print(\"Test data includes:\")\n",
    "    print(\"- Customer 1001: Updated email address (existing customer)\")\n",
    "    print(\"- Customer 1002: No changes (existing customer)\")\n",
    "    print(\"- Customer 1007: New business customer\")\n",
    "    print(\"- Customer 1008: New high-risk customer\")\n",
    "    \n",
    "    # Load the updated data\n",
    "    loader = DatabaseLoader(warehouse_connection_string)\n",
    "    update_results = loader.load_enriched_customers(df_updates)\n",
    "    \n",
    "    print(\"\\n=== UPSERT RESULTS ===\")\n",
    "    print(f\"Total records processed: {update_results['total_records']}\")\n",
    "    print(f\"New customers inserted: {update_results['successful_inserts']}\")\n",
    "    print(f\"Existing customers updated: {update_results['successful_updates']}\")\n",
    "    print(f\"Processing time: {update_results['processing_time']:.2f} seconds\")\n",
    "    \n",
    "    # Verify the results using SQLAlchemy engine\n",
    "    params = urllib.parse.quote_plus(warehouse_connection_string)\n",
    "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "    \n",
    "    verification_query = \"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name + ' ' + last_name as customer_name,\n",
    "        email,\n",
    "        data_source,\n",
    "        created_date,\n",
    "        modified_date\n",
    "    FROM customer_enriched \n",
    "    WHERE customer_id IN (1001, 1002, 1007, 1008)\n",
    "    ORDER BY customer_id\n",
    "    \"\"\"\n",
    "    \n",
    "    verification_df = pd.read_sql(verification_query, engine)\n",
    "    print(\"\\n=== VERIFICATION OF UPSERT RESULTS ===\")\n",
    "    for _, row in verification_df.iterrows():\n",
    "        created = row['created_date'].strftime('%H:%M:%S')\n",
    "        modified = row['modified_date'].strftime('%H:%M:%S')\n",
    "        source = row['data_source']\n",
    "        \n",
    "        if created == modified and 'Update' not in source:\n",
    "            status = \"üÜï NEW\"\n",
    "        elif 'Update' in source:\n",
    "            status = \"‚úèÔ∏è  UPDATED\"\n",
    "        else:\n",
    "            status = \"üîÑ EXISTING\"\n",
    "            \n",
    "        print(f\"   {status} {row['customer_name']} ({row['customer_id']}) - {row['email']}\")\n",
    "    \n",
    "    return update_results\n",
    "\n",
    "# Run the upsert test\n",
    "if validation_success:\n",
    "    upsert_results = test_upsert_functionality()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Skipping upsert test due to validation issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: ETL Pipeline Summary\n",
    "\n",
    "Status report of the full ETL pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to strip emojis from a string\n",
    "import re\n",
    "def remove_emojis(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "# Generate final pipeline summary\n",
    "def generate_pipeline_summary():\n",
    "    \"\"\"Create comprehensive summary of ETL pipeline execution\"\"\"\n",
    "    \n",
    "    summary_report = f\"\"\"\n",
    "==========================================================\n",
    "üéâ ETL PIPELINE EXECUTION COMPLETE!\n",
    "==========================================================\n",
    "\n",
    "PIPELINE OVERVIEW:\n",
    "‚Ä¢ Extract: ‚úÖ Customer data from CSV + API enrichment\n",
    "‚Ä¢ Transform: ‚úÖ Data cleaning + business logic + risk scoring  \n",
    "‚Ä¢ Load: ‚úÖ Enriched data loaded to SQL Server data warehouse\n",
    "\n",
    "TODAY'S ACHIEVEMENTS:\n",
    "‚úÖ Built complete ETL pipeline from scratch\n",
    "‚úÖ Integrated multiple data sources (CSV + APIs)\n",
    "‚úÖ Applied business logic and data validation\n",
    "‚úÖ Implemented production-ready database loading\n",
    "‚úÖ Created audit trails and monitoring\n",
    "‚úÖ Generated business insights from enriched data\n",
    "\n",
    "TECHNICAL COMPONENTS MASTERED:\n",
    "‚Ä¢ Python ETL development (pandas, requests)\n",
    "‚Ä¢ API integration and error handling\n",
    "‚Ä¢ SQL Server database operations\n",
    "‚Ä¢ Data quality assessment and reporting\n",
    "‚Ä¢ Business intelligence and analytics\n",
    "\n",
    "BUSINESS VALUE CREATED:\n",
    "‚Ä¢ Customer support team has enriched profiles\n",
    "‚Ä¢ Risk assessment enables proactive management\n",
    "‚Ä¢ Geographic insights support expansion planning\n",
    "‚Ä¢ Data quality metrics ensure reliability\n",
    "\n",
    "PRODUCTION READINESS:\n",
    "‚Ä¢ ‚úÖ Error handling and graceful failure recovery\n",
    "‚Ä¢ ‚úÖ Audit logging for compliance and monitoring\n",
    "‚Ä¢ ‚úÖ Data validation and quality checks\n",
    "‚Ä¢ ‚úÖ Performance metrics and timing\n",
    "‚Ä¢ ‚úÖ Scalable upsert logic for ongoing updates\n",
    "\n",
    "TOMORROW'S PREVIEW:\n",
    "üîÑ Rebuild this exact pipeline using Azure Data Factory\n",
    "üéØ Learn visual ETL design and enterprise deployment\n",
    "üìä Discover how ETL concepts apply across different tools\n",
    "\n",
    "NEXT STEPS FOR YOUR ORGANISATION:\n",
    "1. Identify data sources that need enrichment\n",
    "2. Map business rules and validation requirements\n",
    "3. Design monitoring and alerting strategies\n",
    "4. Plan for data governance and quality standards\n",
    "\n",
    "üèÜ CONGRATULATIONS!\n",
    "You've successfully completed a professional-grade ETL pipeline!\n",
    "==========================================================\n",
    "\"\"\"\n",
    "    \n",
    "    print(summary_report)\n",
    "    \n",
    "    # Save summary to file\n",
    "    text_for_file = remove_emojis(summary_report)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    summary_file = f'etl_pipeline_summary_{timestamp}.txt'\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(text_for_file)\n",
    "    \n",
    "    print(f\"üìÑ Pipeline summary saved to: {summary_file}\")\n",
    "\n",
    "# Generate the final summary\n",
    "generate_pipeline_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection and Discussion\n",
    "\n",
    "**Production Considerations:**\n",
    "- What monitoring would you implement for this pipeline?\n",
    "- How would you handle the pipeline failing at 3am?\n",
    "- What data governance policies are needed?\n",
    "- How would you secure sensitive customer data?\n",
    "\n",
    "**Tomorrow's Bridge:**\n",
    "- How might visual tools like Azure Data Factory change this process?\n",
    "- What are the pros/cons of coded vs visual ETL approaches?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
