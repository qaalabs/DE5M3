{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Loading - The Load Component\n",
    "\n",
    "### Scenario\n",
    "You've Extracted data from APIs and Transformed it with business logic. Now it's time to Load it into your data warehouse.\n",
    "\n",
    "### Business Context\n",
    "Your customer service team needs this enriched data in their CRM system for better customer support.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from sqlalchemy import create_engine\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Database Setup and Connection\n",
    "\n",
    "Connect to SQL Server and create the customer data warehouse database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection configuration\n",
    "SERVER = 'localhost'  # Your SQL Server instance\n",
    "DATABASE = 'customer_warehouse'  # We'll create this database\n",
    "\n",
    "# Connection string for initial setup (master database)\n",
    "master_connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SERVER};DATABASE=master;Trusted_Connection=yes;'\n",
    "\n",
    "print(\"=== DATABASE SETUP ===\")\n",
    "try:\n",
    "    # Connect to master database to create our warehouse (with autocommit for CREATE DATABASE)\n",
    "    master_conn = pyodbc.connect(master_connection_string, autocommit=True)\n",
    "    master_cursor = master_conn.cursor()\n",
    "    \n",
    "    # Create database if it doesn't exist\n",
    "    try:\n",
    "        master_cursor.execute(f\"CREATE DATABASE {DATABASE}\")\n",
    "        print(f\"✅ Created database: {DATABASE}\")\n",
    "    except pyodbc.Error as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            print(f\"ℹ️  Database {DATABASE} already exists\")\n",
    "        else:\n",
    "            print(f\"⚠️  Database creation issue: {e}\")\n",
    "    \n",
    "    master_conn.close()\n",
    "    print(\"Database setup complete\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Database setup failed: {e}\")\n",
    "    print(\"Please check your SQL Server connection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Data Warehouse Tables\n",
    "\n",
    "Create tables optimized for analytical queries and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to our customer warehouse database\n",
    "warehouse_connection_string = f'DRIVER={{ODBC Driver 17 for SQL Server}};SERVER={SERVER};DATABASE={DATABASE};Trusted_Connection=yes;'\n",
    "\n",
    "def create_warehouse_tables():\n",
    "    \"\"\"Create optimized data warehouse tables\"\"\"\n",
    "    \n",
    "    # Table creation scripts\n",
    "    create_tables_sql = \"\"\"\n",
    "    -- Drop existing tables if they exist\n",
    "    IF OBJECT_ID('dbo.customer_enriched', 'U') IS NOT NULL DROP TABLE dbo.customer_enriched;\n",
    "    IF OBJECT_ID('dbo.enrichment_audit', 'U') IS NOT NULL DROP TABLE dbo.enrichment_audit;\n",
    "    \n",
    "    -- Main customer data table\n",
    "    CREATE TABLE customer_enriched (\n",
    "        customer_id INT PRIMARY KEY,\n",
    "        first_name NVARCHAR(50) NOT NULL,\n",
    "        last_name NVARCHAR(50) NOT NULL,\n",
    "        email NVARCHAR(100) NOT NULL,\n",
    "        phone NVARCHAR(20),\n",
    "        postcode NVARCHAR(10),\n",
    "        \n",
    "        -- Geographic enrichment\n",
    "        region NVARCHAR(50),\n",
    "        country NVARCHAR(50),\n",
    "        district NVARCHAR(50),\n",
    "        longitude DECIMAL(10,7),\n",
    "        latitude DECIMAL(10,7),\n",
    "        geo_enriched BIT DEFAULT 0,\n",
    "        \n",
    "        -- Business enrichment\n",
    "        company NVARCHAR(100),\n",
    "        company_size NVARCHAR(50),\n",
    "        industry NVARCHAR(50),\n",
    "        annual_revenue NVARCHAR(50),\n",
    "        is_business BIT DEFAULT 0,\n",
    "        \n",
    "        -- Risk assessment\n",
    "        calculated_risk NVARCHAR(20),\n",
    "        risk_score_numeric INT,\n",
    "        risk_factors NVARCHAR(500),\n",
    "        \n",
    "        -- Account status\n",
    "        status NVARCHAR(20),\n",
    "        \n",
    "        -- ETL metadata\n",
    "        processed_date DATETIME2 DEFAULT GETDATE(),\n",
    "        data_source NVARCHAR(50),\n",
    "        enrichment_status NVARCHAR(50),\n",
    "        \n",
    "        -- Audit fields\n",
    "        created_date DATETIME2 DEFAULT GETDATE(),\n",
    "        modified_date DATETIME2 DEFAULT GETDATE()\n",
    "    );\n",
    "    \n",
    "    -- Audit table for tracking all loading operations\n",
    "    CREATE TABLE enrichment_audit (\n",
    "        audit_id INT IDENTITY(1,1) PRIMARY KEY,\n",
    "        batch_id UNIQUEIDENTIFIER DEFAULT NEWID(),\n",
    "        operation_type NVARCHAR(20), -- INSERT, UPDATE, DELETE\n",
    "        records_processed INT,\n",
    "        records_successful INT,\n",
    "        records_failed INT,\n",
    "        processing_start DATETIME2,\n",
    "        processing_end DATETIME2,\n",
    "        duration_seconds AS DATEDIFF(SECOND, processing_start, processing_end),\n",
    "        error_message NVARCHAR(1000),\n",
    "        pipeline_version NVARCHAR(20)\n",
    "    );\n",
    "    \n",
    "    -- Create indexes for better query performance\n",
    "    CREATE INDEX IX_customer_enriched_region ON customer_enriched(region);\n",
    "    CREATE INDEX IX_customer_enriched_risk ON customer_enriched(calculated_risk);\n",
    "    CREATE INDEX IX_customer_enriched_business ON customer_enriched(is_business);\n",
    "    CREATE INDEX IX_customer_enriched_status ON customer_enriched(status);\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        conn = pyodbc.connect(warehouse_connection_string)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Execute table creation\n",
    "        cursor.execute(create_tables_sql)\n",
    "        conn.commit()\n",
    "        \n",
    "        print(\"✅ Data warehouse tables created successfully\")\n",
    "        print(\"   - customer_enriched (main data table)\")\n",
    "        print(\"   - enrichment_audit (processing audit trail)\")\n",
    "        print(\"   - Performance indexes created\")\n",
    "        \n",
    "        conn.close()\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Table creation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Create the tables\n",
    "print(\"=== CREATING DATA WAREHOUSE TABLES ===\")\n",
    "table_creation_success = create_warehouse_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Sample Enriched Data\n",
    "\n",
    "For this proof-ofconcept, create sample enriched data\n",
    "\n",
    "**#TODO** Extract this data from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample enriched customer data (from our API enrichment lab)\n",
    "enriched_customers = {\n",
    "    'customer_id': [1001, 1002, 1003, 1004, 1005, 1006],\n",
    "    'first_name': ['John', 'Jane', 'Mike', 'Sarah', 'Bob', 'Alice'],\n",
    "    'last_name': ['Smith', 'Doe', 'Johnson', 'Wilson', 'Brown', 'Cooper'],\n",
    "    'email': ['john@email.com', 'jane@email.com', 'mike@techcorp.com', \n",
    "              'sarah@retailplus.com', 'bob@email.com', 'alice@freelance.com'],\n",
    "    'phone': ['01234567890', '01987654321', '01555123456', \n",
    "              '01777888999', '01111222333', '01444555666'],\n",
    "    'postcode': ['SW1A 1AA', 'M1 1AF', 'B1 1BB', 'LS1 2AJ', 'NE1 3NG', 'CF10 2HH'],\n",
    "    \n",
    "    # Geographic enrichment\n",
    "    'region': ['London', 'North West', 'West Midlands', 'Yorkshire and The Humber', 'North East', 'Wales'],\n",
    "    'country': ['England', 'England', 'England', 'England', 'England', 'Wales'],\n",
    "    'district': ['Westminster', 'Manchester', 'Birmingham', 'Leeds', 'Newcastle', 'Cardiff'],\n",
    "    'longitude': [-0.1419, -2.2426, -1.8904, -1.5491, -1.6131, -3.1791],\n",
    "    'latitude': [51.5014, 53.4794, 52.4796, 53.7997, 54.9738, 51.4816],\n",
    "    'geo_enriched': [1, 1, 1, 1, 1, 1],  # Changed from True to 1 for SQL BIT compatibility\n",
    "    \n",
    "    # Business enrichment\n",
    "    'company': ['', '', 'TechCorp Ltd', 'Retail Plus', '', 'Freelance Design'],\n",
    "    'company_size': ['Individual', 'Individual', 'Medium (50-250 employees)', 'Large (250+ employees)', 'Individual', 'Micro (1-10 employees)'],\n",
    "    'industry': ['Personal', 'Personal', 'Technology', 'Retail', 'Personal', 'Creative Services'],\n",
    "    'annual_revenue': ['N/A', 'N/A', '£2M-£10M', '£10M+', 'N/A', '£0-£100K'],\n",
    "    'is_business': [0, 0, 1, 1, 0, 1],  # Changed from False/True to 0/1 for SQL BIT compatibility\n",
    "    \n",
    "    # Risk assessment\n",
    "    'calculated_risk': ['Low', 'Low', 'Medium', 'High', 'Low', 'Medium'],\n",
    "    'risk_score_numeric': [0, 0, 2, 5, 0, 1],\n",
    "    'risk_factors': ['Standard profile', 'Standard profile', 'High-risk region', \n",
    "                    'Account suspended; High-risk region', 'Standard profile', 'Small business'],\n",
    "    \n",
    "    # Account status\n",
    "    'status': ['active', 'active', 'active', 'suspended', 'active', 'active'],\n",
    "    \n",
    "    # ETL metadata\n",
    "    'processed_date': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')] * 6,\n",
    "    'data_source': ['ETL_Pipeline_v1'] * 6,\n",
    "    'enrichment_status': ['Fully Enriched', 'Fully Enriched', 'Fully Enriched', \n",
    "                         'Fully Enriched', 'Fully Enriched', 'Fully Enriched']\n",
    "}\n",
    "\n",
    "df_enriched = pd.DataFrame(enriched_customers)\n",
    "\n",
    "print(\"=== ENRICHED CUSTOMER DATA TO LOAD ===\")\n",
    "print(f\"Records to load: {len(df_enriched)}\")\n",
    "print(\"\\nSample records:\")\n",
    "display_cols = ['customer_id', 'first_name', 'last_name', 'region', 'industry', 'calculated_risk']\n",
    "print(df_enriched[display_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Implement Robust Database Loading\n",
    "\n",
    "Create a production-ready loading process with error handling, audit logging, and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from typing import Tuple, Dict\n",
    "\n",
    "class DatabaseLoader:\n",
    "    \"\"\"Production database loading with comprehensive error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, connection_string: str):\n",
    "        self.connection_string = connection_string\n",
    "        self.batch_id = str(uuid.uuid4())\n",
    "        \n",
    "    def load_enriched_customers(self, df_customers: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Load enriched customer data with comprehensive error handling\n",
    "        Returns: Loading statistics and results\n",
    "        \"\"\"\n",
    "        start_time = datetime.now()\n",
    "        results = {\n",
    "            'batch_id': self.batch_id,\n",
    "            'total_records': len(df_customers),\n",
    "            'successful_inserts': 0,\n",
    "            'successful_updates': 0,\n",
    "            'failed_records': 0,\n",
    "            'errors': [],\n",
    "            'processing_time': 0\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            conn = pyodbc.connect(self.connection_string)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            print(f\"🔄 Starting batch load: {self.batch_id}\")\n",
    "            print(f\"📊 Processing {len(df_customers)} customer records\")\n",
    "            \n",
    "            # Process each customer with upsert logic\n",
    "            for index, customer in df_customers.iterrows():\n",
    "                try:\n",
    "                    # Check if customer already exists\n",
    "                    check_sql = \"SELECT COUNT(*) FROM customer_enriched WHERE customer_id = ?\"\n",
    "                    cursor.execute(check_sql, customer['customer_id'])\n",
    "                    exists = cursor.fetchone()[0] > 0\n",
    "                    \n",
    "                    if exists:\n",
    "                        # UPDATE existing record\n",
    "                        update_sql = \"\"\"\n",
    "                        UPDATE customer_enriched SET\n",
    "                            first_name = ?, last_name = ?, email = ?, phone = ?, postcode = ?,\n",
    "                            region = ?, country = ?, district = ?, longitude = ?, latitude = ?, geo_enriched = ?,\n",
    "                            company = ?, company_size = ?, industry = ?, annual_revenue = ?, is_business = ?,\n",
    "                            calculated_risk = ?, risk_score_numeric = ?, risk_factors = ?,\n",
    "                            status = ?, processed_date = ?, data_source = ?, enrichment_status = ?,\n",
    "                            modified_date = GETDATE()\n",
    "                        WHERE customer_id = ?\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        cursor.execute(update_sql, (\n",
    "                            customer['first_name'], customer['last_name'], customer['email'],\n",
    "                            customer['phone'], customer['postcode'],\n",
    "                            customer['region'], customer['country'], customer['district'],\n",
    "                            customer['longitude'], customer['latitude'], customer['geo_enriched'],\n",
    "                            customer['company'], customer['company_size'], customer['industry'],\n",
    "                            customer['annual_revenue'], customer['is_business'],\n",
    "                            customer['calculated_risk'], customer['risk_score_numeric'], customer['risk_factors'],\n",
    "                            customer['status'], customer['processed_date'], customer['data_source'],\n",
    "                            customer['enrichment_status'], customer['customer_id']\n",
    "                        ))\n",
    "                        \n",
    "                        results['successful_updates'] += 1\n",
    "                        print(f\"  ✏️  Updated customer {customer['customer_id']}\")\n",
    "                        \n",
    "                    else:\n",
    "                        # INSERT new record\n",
    "                        insert_sql = \"\"\"\n",
    "                        INSERT INTO customer_enriched (\n",
    "                            customer_id, first_name, last_name, email, phone, postcode,\n",
    "                            region, country, district, longitude, latitude, geo_enriched,\n",
    "                            company, company_size, industry, annual_revenue, is_business,\n",
    "                            calculated_risk, risk_score_numeric, risk_factors,\n",
    "                            status, processed_date, data_source, enrichment_status\n",
    "                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                        \"\"\"\n",
    "                        \n",
    "                        cursor.execute(insert_sql, (\n",
    "                            customer['customer_id'], customer['first_name'], customer['last_name'],\n",
    "                            customer['email'], customer['phone'], customer['postcode'],\n",
    "                            customer['region'], customer['country'], customer['district'],\n",
    "                            customer['longitude'], customer['latitude'], customer['geo_enriched'],\n",
    "                            customer['company'], customer['company_size'], customer['industry'],\n",
    "                            customer['annual_revenue'], customer['is_business'],\n",
    "                            customer['calculated_risk'], customer['risk_score_numeric'], customer['risk_factors'],\n",
    "                            customer['status'], customer['processed_date'], customer['data_source'],\n",
    "                            customer['enrichment_status']\n",
    "                        ))\n",
    "                        \n",
    "                        results['successful_inserts'] += 1\n",
    "                        print(f\"  ➕ Inserted customer {customer['customer_id']}\")\n",
    "                \n",
    "                except Exception as record_error:\n",
    "                    error_msg = f\"Customer {customer['customer_id']}: {str(record_error)}\"\n",
    "                    results['errors'].append(error_msg)\n",
    "                    results['failed_records'] += 1\n",
    "                    print(f\"  ❌ Failed: {error_msg}\")\n",
    "                    \n",
    "            # Commit all changes\n",
    "            conn.commit()\n",
    "            \n",
    "            # Record processing time\n",
    "            end_time = datetime.now()\n",
    "            results['processing_time'] = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            # Log audit information\n",
    "            self._log_audit_record(cursor, start_time, end_time, results)\n",
    "            conn.commit()\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            print(f\"\\n✅ Batch load completed successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results['errors'].append(f\"Critical loading error: {str(e)}\")\n",
    "            print(f\"❌ Critical loading error: {e}\")\n",
    "            \n",
    "        return results\n",
    "    \n",
    "    def _log_audit_record(self, cursor, start_time, end_time, results):\n",
    "        \"\"\"Log detailed audit information\"\"\"\n",
    "        audit_sql = \"\"\"\n",
    "        INSERT INTO enrichment_audit (\n",
    "            batch_id, operation_type, records_processed, records_successful, \n",
    "            records_failed, processing_start, processing_end, error_message, pipeline_version\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        \n",
    "        error_summary = '; '.join(results['errors'][:5]) if results['errors'] else None\n",
    "        successful_records = results['successful_inserts'] + results['successful_updates']\n",
    "        \n",
    "        cursor.execute(audit_sql, (\n",
    "            self.batch_id,\n",
    "            'UPSERT',\n",
    "            results['total_records'],\n",
    "            successful_records,\n",
    "            results['failed_records'],\n",
    "            start_time,\n",
    "            end_time,\n",
    "            error_summary,\n",
    "            'ETL_Pipeline_v1.0'\n",
    "        ))\n",
    "\n",
    "# Execute the loading process\n",
    "print(\"=== LOADING ENRICHED DATA TO DATABASE ===\")\n",
    "\n",
    "if table_creation_success:\n",
    "    loader = DatabaseLoader(warehouse_connection_string)\n",
    "    loading_results = loader.load_enriched_customers(df_enriched)\n",
    "    \n",
    "    print(\"\\n=== LOADING RESULTS ===\")\n",
    "    print(f\"Batch ID: {loading_results['batch_id']}\")\n",
    "    print(f\"Total records: {loading_results['total_records']}\")\n",
    "    print(f\"Successful inserts: {loading_results['successful_inserts']}\")\n",
    "    print(f\"Successful updates: {loading_results['successful_updates']}\")\n",
    "    print(f\"Failed records: {loading_results['failed_records']}\")\n",
    "    print(f\"Processing time: {loading_results['processing_time']:.2f} seconds\")\n",
    "    \n",
    "    if loading_results['errors']:\n",
    "        print(\"\\nErrors encountered:\")\n",
    "        for error in loading_results['errors']:\n",
    "            print(f\"  ⚠️  {error}\")\n",
    "else:\n",
    "    print(\"❌ Skipping data loading due to table creation failure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Data Validation and Quality Checks\n",
    "\n",
    "Verify that the data was loaded correctly and perform quality checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_loaded_data():\n",
    "    \"\"\"Comprehensive validation of loaded data\"\"\"\n",
    "    \n",
    "    # Set up SQLAlchemy engine for pandas compatibility\n",
    "    params = urllib.parse.quote_plus(warehouse_connection_string)\n",
    "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"=== DATA VALIDATION CHECKS ===\")\n",
    "        \n",
    "        # 1. Record count validation\n",
    "        count_query = \"SELECT COUNT(*) FROM customer_enriched\"\n",
    "        total_records = pd.read_sql(count_query, engine).iloc[0, 0]\n",
    "        print(f\"✅ Total records in database: {total_records}\")\n",
    "        \n",
    "        # 2. Data completeness checks\n",
    "        completeness_query = \"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_records,\n",
    "            SUM(CASE WHEN first_name IS NOT NULL AND first_name != '' THEN 1 ELSE 0 END) as complete_names,\n",
    "            SUM(CASE WHEN email IS NOT NULL AND email != '' THEN 1 ELSE 0 END) as complete_emails,\n",
    "            SUM(CASE WHEN geo_enriched = 1 THEN 1 ELSE 0 END) as geo_enriched_count,\n",
    "            SUM(CASE WHEN is_business = 1 THEN 1 ELSE 0 END) as business_customers\n",
    "        FROM customer_enriched\n",
    "        \"\"\"\n",
    "        \n",
    "        completeness_df = pd.read_sql(completeness_query, engine)\n",
    "        comp = completeness_df.iloc[0]\n",
    "        \n",
    "        print(f\"✅ Name completeness: {comp['complete_names']}/{comp['total_records']} ({comp['complete_names']/comp['total_records']:.1%})\")\n",
    "        print(f\"✅ Email completeness: {comp['complete_emails']}/{comp['total_records']} ({comp['complete_emails']/comp['total_records']:.1%})\")\n",
    "        print(f\"✅ Geographic enrichment: {comp['geo_enriched_count']}/{comp['total_records']} ({comp['geo_enriched_count']/comp['total_records']:.1%})\")\n",
    "        print(f\"✅ Business customers: {comp['business_customers']}/{comp['total_records']} ({comp['business_customers']/comp['total_records']:.1%})\")\n",
    "        \n",
    "        # 3. Risk distribution analysis\n",
    "        risk_query = \"\"\"\n",
    "        SELECT \n",
    "            calculated_risk,\n",
    "            COUNT(*) as customer_count,\n",
    "            CAST(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER() AS DECIMAL(5,1)) as percentage\n",
    "        FROM customer_enriched \n",
    "        GROUP BY calculated_risk\n",
    "        ORDER BY customer_count DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        risk_df = pd.read_sql(risk_query, engine)\n",
    "        print(f\"\\n✅ Risk Distribution:\")\n",
    "        for _, row in risk_df.iterrows():\n",
    "            print(f\"   {row['calculated_risk']} Risk: {row['customer_count']} customers ({row['percentage']}%)\")\n",
    "        \n",
    "        # 4. Geographic distribution\n",
    "        geo_query = \"\"\"\n",
    "        SELECT TOP 5\n",
    "            region,\n",
    "            COUNT(*) as customer_count\n",
    "        FROM customer_enriched \n",
    "        WHERE region IS NOT NULL AND region != 'Unknown'\n",
    "        GROUP BY region\n",
    "        ORDER BY customer_count DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        geo_df = pd.read_sql(geo_query, engine)\n",
    "        print(f\"\\n✅ Top Regions by Customer Count:\")\n",
    "        for _, row in geo_df.iterrows():\n",
    "            print(f\"   {row['region']}: {row['customer_count']} customers\")\n",
    "        \n",
    "        # 5. Audit trail verification\n",
    "        audit_query = \"\"\"\n",
    "        SELECT \n",
    "            batch_id,\n",
    "            operation_type,\n",
    "            records_processed,\n",
    "            records_successful,\n",
    "            records_failed,\n",
    "            duration_seconds,\n",
    "            processing_start\n",
    "        FROM enrichment_audit \n",
    "        ORDER BY processing_start DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        audit_df = pd.read_sql(audit_query, engine)\n",
    "        print(f\"\\n✅ Recent Processing Batches:\")\n",
    "        for _, row in audit_df.iterrows():\n",
    "            success_rate = (row['records_successful'] / row['records_processed'] * 100) if row['records_processed'] > 0 else 0\n",
    "            print(f\"   Batch: {str(row['batch_id'])[:8]}... | {row['records_processed']} records | {success_rate:.1f}% success | {row['duration_seconds']}s\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Validation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Run validation\n",
    "validation_success = validate_loaded_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Business Intelligence Queries\n",
    "\n",
    "Demonstrate the value of enriched data with business-relevant queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_business_insights():\n",
    "    \"\"\"Generate business insights from enriched customer data\"\"\"\n",
    "    \n",
    "    # Set up SQLAlchemy engine for pandas compatibility\n",
    "    params = urllib.parse.quote_plus(warehouse_connection_string)\n",
    "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"=== BUSINESS INSIGHTS FROM ENRICHED DATA ===\")\n",
    "        \n",
    "        # 1. High-value business customers by region\n",
    "        high_value_query = \"\"\"\n",
    "        SELECT \n",
    "            region,\n",
    "            COUNT(*) as business_customers,\n",
    "            SUM(CASE WHEN calculated_risk = 'Low' THEN 1 ELSE 0 END) as low_risk_businesses\n",
    "        FROM customer_enriched \n",
    "        WHERE is_business = 1\n",
    "        GROUP BY region\n",
    "        ORDER BY business_customers DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        high_value_df = pd.read_sql(high_value_query, engine)\n",
    "        print(\"\\n📊 Business Customers by Region:\")\n",
    "        for _, row in high_value_df.iterrows():\n",
    "            low_risk_pct = (row['low_risk_businesses'] / row['business_customers'] * 100) if row['business_customers'] > 0 else 0\n",
    "            print(f\"   {row['region']}: {row['business_customers']} businesses ({row['low_risk_businesses']} low-risk, {low_risk_pct:.0f}%)\")\n",
    "        \n",
    "        # 2. Risk assessment for customer support prioritization\n",
    "        support_priority_query = \"\"\"\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            first_name + ' ' + last_name as customer_name,\n",
    "            company,\n",
    "            region,\n",
    "            calculated_risk,\n",
    "            risk_factors,\n",
    "            status\n",
    "        FROM customer_enriched \n",
    "        WHERE calculated_risk IN ('High', 'Medium')\n",
    "        ORDER BY \n",
    "            CASE calculated_risk WHEN 'High' THEN 1 WHEN 'Medium' THEN 2 ELSE 3 END,\n",
    "            customer_name\n",
    "        \"\"\"\n",
    "        \n",
    "        priority_df = pd.read_sql(support_priority_query, engine)\n",
    "        print(f\"\\n🚨 Customer Support Priority List ({len(priority_df)} customers):\")\n",
    "        for _, row in priority_df.iterrows():\n",
    "            risk_icon = \"🔴\" if row['calculated_risk'] == 'High' else \"🟡\"\n",
    "            company_info = f\" ({row['company']})\" if row['company'] else \"\"\n",
    "            print(f\"   {risk_icon} {row['customer_name']}{company_info} - {row['region']} - {row['risk_factors']}\")\n",
    "        \n",
    "        # 3. Geographic expansion opportunities\n",
    "        expansion_query = \"\"\"\n",
    "        SELECT \n",
    "            region,\n",
    "            COUNT(*) as total_customers,\n",
    "            SUM(CASE WHEN is_business = 1 THEN 1 ELSE 0 END) as business_customers,\n",
    "            SUM(CASE WHEN status = 'active' THEN 1 ELSE 0 END) as active_customers,\n",
    "            AVG(CAST(risk_score_numeric AS FLOAT)) as avg_risk_score\n",
    "        FROM customer_enriched \n",
    "        GROUP BY region\n",
    "        ORDER BY total_customers DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        expansion_df = pd.read_sql(expansion_query, engine)\n",
    "        print(f\"\\n🎯 Market Analysis by Region:\")\n",
    "        for _, row in expansion_df.iterrows():\n",
    "            business_pct = (row['business_customers'] / row['total_customers'] * 100) if row['total_customers'] > 0 else 0\n",
    "            active_pct = (row['active_customers'] / row['total_customers'] * 100) if row['total_customers'] > 0 else 0\n",
    "            print(f\"   {row['region']}: {row['total_customers']} customers | {business_pct:.0f}% business | {active_pct:.0f}% active | Risk: {row['avg_risk_score']:.1f}\")\n",
    "        \n",
    "        # 4. Data quality scorecard\n",
    "        quality_query = \"\"\"\n",
    "        SELECT \n",
    "            enrichment_status,\n",
    "            COUNT(*) as customer_count,\n",
    "            AVG(CASE WHEN geo_enriched = 1 THEN 100.0 ELSE 0.0 END) as geo_completion_rate,\n",
    "            AVG(CASE WHEN is_business = 1 AND company IS NOT NULL AND company != '' THEN 100.0 \n",
    "                     WHEN is_business = 0 THEN 100.0 ELSE 0.0 END) as business_data_quality\n",
    "        FROM customer_enriched \n",
    "        GROUP BY enrichment_status\n",
    "        ORDER BY customer_count DESC\n",
    "        \"\"\"\n",
    "        \n",
    "        quality_df = pd.read_sql(quality_query, engine)\n",
    "        print(f\"\\n📈 Data Quality Scorecard:\")\n",
    "        for _, row in quality_df.iterrows():\n",
    "            print(f\"   {row['enrichment_status']}: {row['customer_count']} customers | Geo: {row['geo_completion_rate']:.0f}% | Business: {row['business_data_quality']:.0f}%\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Business insights generation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Generate insights\n",
    "if validation_success:\n",
    "    insights_success = generate_business_insights()\n",
    "else:\n",
    "    print(\"⚠️  Skipping business insights due to validation issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test Upsert Functionality\n",
    "\n",
    "Demonstrate how the pipeline handles updates to existing customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test upsert functionality with updated customer data\n",
    "def test_upsert_functionality():\n",
    "    \"\"\"Test how the pipeline handles existing customer updates\"\"\"\n",
    "    \n",
    "    print(\"=== TESTING UPSERT FUNCTIONALITY ===\")\n",
    "    \n",
    "    # Create updated customer data (some existing, some new)\n",
    "    updated_customers = {\n",
    "        'customer_id': [1001, 1002, 1007, 1008],  # 1001,1002 exist, 1007,1008 are new\n",
    "        'first_name': ['John', 'Jane', 'David', 'Emma'],\n",
    "        'last_name': ['Smith', 'Doe', 'Taylor', 'Watson'],\n",
    "        'email': ['john.smith@newemail.com', 'jane@email.com', 'david@company.com', 'emma@startup.com'],\n",
    "        'phone': ['01234567890', '01987654321', '01666777888', '01999888777'],\n",
    "        'postcode': ['SW1A 1AA', 'M1 1AH', 'E1 0AD', 'EC1A 1BB'],\n",
    "        \n",
    "        # Geographic enrichment\n",
    "        'region': ['London', 'North West', 'London', 'London'],\n",
    "        'country': ['England', 'England', 'England', 'England'],\n",
    "        'district': ['Westminster', 'Manchester', 'Tower Hamlets', 'City of London'],\n",
    "        'longitude': [-0.1419, -2.2426, -0.0713, -0.0982],\n",
    "        'latitude': [51.5014, 53.4794, 51.5206, 51.5155],\n",
    "        'geo_enriched': [1, 1, 1, 1],  # Changed from True to 1\n",
    "        \n",
    "        # Business enrichment\n",
    "        'company': ['', '', 'Tech Solutions Ltd', 'Innovation Startup'],\n",
    "        'company_size': ['Individual', 'Individual', 'Small (10-50 employees)', 'Micro (1-10 employees)'],\n",
    "        'industry': ['Personal', 'Personal', 'Technology', 'Technology'],\n",
    "        'annual_revenue': ['N/A', 'N/A', '£100K-£2M', '£0-£100K'],\n",
    "        'is_business': [0, 0, 1, 1],  # Changed from False/True to 0/1\n",
    "        \n",
    "        # Risk assessment\n",
    "        'calculated_risk': ['Low', 'Low', 'Medium', 'High'],\n",
    "        'risk_score_numeric': [0, 0, 2, 3],\n",
    "        'risk_factors': ['Standard profile', 'Standard profile', 'High-risk region', \n",
    "                        'New business; High-risk region'],\n",
    "        \n",
    "        # Account status\n",
    "        'status': ['active', 'active', 'active', 'active'],\n",
    "        \n",
    "        # ETL metadata\n",
    "        'processed_date': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')] * 4,\n",
    "        'data_source': ['ETL_Pipeline_v1_Update'] * 4,\n",
    "        'enrichment_status': ['Fully Enriched'] * 4\n",
    "    }\n",
    "    \n",
    "    df_updates = pd.DataFrame(updated_customers)\n",
    "    \n",
    "    print(\"Test data includes:\")\n",
    "    print(\"- Customer 1001: Updated email address (existing customer)\")\n",
    "    print(\"- Customer 1002: No changes (existing customer)\")\n",
    "    print(\"- Customer 1007: New business customer\")\n",
    "    print(\"- Customer 1008: New high-risk customer\")\n",
    "    \n",
    "    # Load the updated data\n",
    "    loader = DatabaseLoader(warehouse_connection_string)\n",
    "    update_results = loader.load_enriched_customers(df_updates)\n",
    "    \n",
    "    print(\"\\n=== UPSERT RESULTS ===\")\n",
    "    print(f\"Total records processed: {update_results['total_records']}\")\n",
    "    print(f\"New customers inserted: {update_results['successful_inserts']}\")\n",
    "    print(f\"Existing customers updated: {update_results['successful_updates']}\")\n",
    "    print(f\"Processing time: {update_results['processing_time']:.2f} seconds\")\n",
    "    \n",
    "    # Verify the results using SQLAlchemy engine\n",
    "    params = urllib.parse.quote_plus(warehouse_connection_string)\n",
    "    engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n",
    "    \n",
    "    verification_query = \"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name + ' ' + last_name as customer_name,\n",
    "        email,\n",
    "        data_source,\n",
    "        created_date,\n",
    "        modified_date\n",
    "    FROM customer_enriched \n",
    "    WHERE customer_id IN (1001, 1002, 1007, 1008)\n",
    "    ORDER BY customer_id\n",
    "    \"\"\"\n",
    "    \n",
    "    verification_df = pd.read_sql(verification_query, engine)\n",
    "    print(\"\\n=== VERIFICATION OF UPSERT RESULTS ===\")\n",
    "    for _, row in verification_df.iterrows():\n",
    "        created = row['created_date'].strftime('%H:%M:%S')\n",
    "        modified = row['modified_date'].strftime('%H:%M:%S')\n",
    "        source = row['data_source']\n",
    "        \n",
    "        if created == modified and 'Update' not in source:\n",
    "            status = \"🆕 NEW\"\n",
    "        elif 'Update' in source:\n",
    "            status = \"✏️  UPDATED\"\n",
    "        else:\n",
    "            status = \"🔄 EXISTING\"\n",
    "            \n",
    "        print(f\"   {status} {row['customer_name']} ({row['customer_id']}) - {row['email']}\")\n",
    "    \n",
    "    return update_results\n",
    "\n",
    "# Run the upsert test\n",
    "if validation_success:\n",
    "    upsert_results = test_upsert_functionality()\n",
    "else:\n",
    "    print(\"⚠️  Skipping upsert test due to validation issues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: ETL Pipeline Summary\n",
    "\n",
    "Status report of the full ETL pipeline!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to strip emojis from a string\n",
    "import re\n",
    "def remove_emojis(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "\n",
    "# Generate final pipeline summary\n",
    "def generate_pipeline_summary():\n",
    "    \"\"\"Create comprehensive summary of ETL pipeline execution\"\"\"\n",
    "    \n",
    "    summary_report = f\"\"\"\n",
    "==========================================================\n",
    "🎉 ETL PIPELINE EXECUTION COMPLETE!\n",
    "==========================================================\n",
    "\n",
    "PIPELINE OVERVIEW:\n",
    "• Extract: ✅ Customer data from CSV + API enrichment\n",
    "• Transform: ✅ Data cleaning + business logic + risk scoring  \n",
    "• Load: ✅ Enriched data loaded to SQL Server data warehouse\n",
    "\n",
    "TODAY'S ACHIEVEMENTS:\n",
    "✅ Built complete ETL pipeline from scratch\n",
    "✅ Integrated multiple data sources (CSV + APIs)\n",
    "✅ Applied business logic and data validation\n",
    "✅ Implemented production-ready database loading\n",
    "✅ Created audit trails and monitoring\n",
    "✅ Generated business insights from enriched data\n",
    "\n",
    "TECHNICAL COMPONENTS MASTERED:\n",
    "• Python ETL development (pandas, requests)\n",
    "• API integration and error handling\n",
    "• SQL Server database operations\n",
    "• Data quality assessment and reporting\n",
    "• Business intelligence and analytics\n",
    "\n",
    "BUSINESS VALUE CREATED:\n",
    "• Customer support team has enriched profiles\n",
    "• Risk assessment enables proactive management\n",
    "• Geographic insights support expansion planning\n",
    "• Data quality metrics ensure reliability\n",
    "\n",
    "PRODUCTION READINESS:\n",
    "• ✅ Error handling and graceful failure recovery\n",
    "• ✅ Audit logging for compliance and monitoring\n",
    "• ✅ Data validation and quality checks\n",
    "• ✅ Performance metrics and timing\n",
    "• ✅ Scalable upsert logic for ongoing updates\n",
    "\n",
    "TOMORROW'S PREVIEW:\n",
    "🔄 Rebuild this exact pipeline using Azure Data Factory\n",
    "🎯 Learn visual ETL design and enterprise deployment\n",
    "📊 Discover how ETL concepts apply across different tools\n",
    "\n",
    "NEXT STEPS FOR YOUR ORGANISATION:\n",
    "1. Identify data sources that need enrichment\n",
    "2. Map business rules and validation requirements\n",
    "3. Design monitoring and alerting strategies\n",
    "4. Plan for data governance and quality standards\n",
    "\n",
    "🏆 CONGRATULATIONS!\n",
    "You've successfully completed a professional-grade ETL pipeline!\n",
    "==========================================================\n",
    "\"\"\"\n",
    "    \n",
    "    print(summary_report)\n",
    "    \n",
    "    # Save summary to file\n",
    "    text_for_file = remove_emojis(summary_report)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    summary_file = f'etl_pipeline_summary_{timestamp}.txt'\n",
    "    \n",
    "    with open(summary_file, 'w') as f:\n",
    "        f.write(text_for_file)\n",
    "    \n",
    "    print(f\"📄 Pipeline summary saved to: {summary_file}\")\n",
    "\n",
    "# Generate the final summary\n",
    "generate_pipeline_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Reflection and Discussion\n",
    "\n",
    "**Production Considerations:**\n",
    "- What monitoring would you implement for this pipeline?\n",
    "- How would you handle the pipeline failing at 3am?\n",
    "- What data governance policies are needed?\n",
    "- How would you secure sensitive customer data?\n",
    "\n",
    "**Tomorrow's Bridge:**\n",
    "- How might visual tools like Azure Data Factory change this process?\n",
    "- What are the pros/cons of coded vs visual ETL approaches?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
