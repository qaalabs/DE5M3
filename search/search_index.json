{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome Module 3","text":""},{"location":"404/","title":"Page Not Found","text":"<p>Sorry, the page you're looking for does not exist.</p> <ul> <li>Check the URL for typos</li> <li>Go back to the home page</li> </ul>"},{"location":"resources/","title":"Resources","text":""},{"location":"resources/#links","title":"Links","text":"<ul> <li>Learner reactions ~ Ticks &amp; Crosses</li> </ul>"},{"location":"resources/#qa-platform-labs","title":"QA Platform Labs","text":"<ul> <li>Microsoft Fabric Playground</li> </ul>"},{"location":"trainer/","title":"Trainer Notes","text":""},{"location":"trainer/#day-1-complete-elt-pipeline-in-python","title":"Day 1 - Complete ELT Pipeline in Python","text":""},{"location":"trainer/#session-1","title":"Session 1","text":"<ul> <li>ETL: Focus on Transform ~ Setup</li> <li>ETL Lab: Focus on Transform</li> </ul>"},{"location":"trainer/#session-2","title":"Session 2","text":"<ul> <li>Discussion: Automation in the workplace</li> <li>ETL: Focus on Extract ~ Setup</li> <li>ETL Lab: Focus on Extract</li> </ul>"},{"location":"trainer/#session-3","title":"Session 3","text":"<ul> <li>Discussion: Introductions</li> <li>ETL: Focus on Load ~ Setup</li> <li>ETL Lab: Focus on Load</li> </ul>"},{"location":"trainer/#session-4","title":"Session 4","text":"<ul> <li>ETL Lab: Full ETL Pipeline</li> <li>Discussion: End of day reflections</li> </ul>"},{"location":"trainer/#day-2-microsoft-fabric","title":"Day 2 - Microsoft Fabric","text":""},{"location":"trainer/#session-1_1","title":"Session 1","text":"<ul> <li>Introduction to Microsoft Fabric</li> <li>Practice: Lab 2.1 ~ 01 Create Fabric Lakehouse</li> <li>Discussion: Debrief about the lab</li> </ul>"},{"location":"trainer/#session-2_1","title":"Session 2","text":"<ul> <li>Investigation: Architecture Investigation</li> <li>Investigation: Architecture ~ Report Back</li> <li>Practice: Lab 2.2 ~ 05 Dataflows Gen2</li> <li>Discussion: Debrief about the lab</li> </ul>"},{"location":"trainer/#session-3_1","title":"Session 3","text":"<ul> <li>Practice: Lab 2.3 ~ 04 Ingest Pipeline</li> <li>Discussion: Debrief about the lab</li> </ul>"},{"location":"trainer/#session-4_1","title":"Session 4","text":"<ul> <li>Investigation: ETL Product Investigation</li> <li>Report-Back: ETL Investigation ~ Report back</li> </ul>"},{"location":"trainer/#day-3-microsoft-fabric","title":"Day 3 - Microsoft Fabric","text":""},{"location":"trainer/#session-1_2","title":"Session 1","text":""},{"location":"trainer/#session-2_2","title":"Session 2","text":""},{"location":"trainer/#session-3_2","title":"Session 3","text":""},{"location":"trainer/#session-4_2","title":"Session 4","text":""},{"location":"trainer/#day-4-etl-patterns","title":"Day 4 - ETL Patterns","text":""},{"location":"trainer/#session-1_3","title":"Session 1","text":""},{"location":"trainer/#session-2_3","title":"Session 2","text":""},{"location":"trainer/#session-3_3","title":"Session 3","text":""},{"location":"trainer/#session-4_3","title":"Session 4","text":""},{"location":"agenda/day1/","title":"Day 1 - Complete ELT Pipeline in Python","text":""},{"location":"agenda/day1/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome to: DE5 Module 3 ~ Day 1 (10 mins)</li> <li><code>09:40</code> VM Setup (20 mins)</li> <li><code>10:00</code> ETL: Focus on Transform ~ Setup (10 mins)</li> <li><code>10:10</code> ETL Lab: Focus on Transform (40 mins)</li> </ul>"},{"location":"agenda/day1/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day1/#session-2","title":"Session 2","text":"<ul> <li><code>11:10</code> Discussion: Automation in the workplace (10 mins)</li> <li><code>11:20</code> ETL: Focus on Extract ~ Setup (10 mins)</li> <li><code>11:30</code> ETL Lab: Focus on Extract (50 mins)</li> </ul>"},{"location":"agenda/day1/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day1/#session-3","title":"Session 3","text":"<ul> <li><code>13:20</code> Discussion: Introductions (20 mins)</li> <li><code>13:40</code> ETL: Focus on Load ~ Setup (10 mins)</li> <li><code>13:50</code> ETL Lab: Focus on Load (40 mins)</li> </ul>"},{"location":"agenda/day1/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day1/#session-4","title":"Session 4","text":"<ul> <li><code>14:50</code> ETL Lab: Full ETL Pipeline (50 mins)</li> <li><code>15:40</code> Discussion: End of day reflections (10 mins)</li> <li><code>15:50</code> Wrap up (10 mins)</li> </ul>"},{"location":"agenda/day2/","title":"Day 2 - Microsoft Fabric","text":""},{"location":"agenda/day2/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome: Welcome to: DE5 Module 3 ~ Day 2 (10 mins)</li> <li><code>09:40</code> VM Setup (10 mins)</li> <li><code>09:50</code> Introduction to Microsoft Fabric (10 mins)</li> <li><code>10:00</code> Practice: Lab 2.1 ~ 01 Create Fabric Lakehouse (30 mins)</li> <li><code>10:30</code> Discussion: Debrief about the lab (10 mins)</li> </ul>"},{"location":"agenda/day2/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day2/#session-2","title":"Session 2","text":"<ul> <li><code>11:00</code> Investigation: Architecture Investigation (20 mins)</li> <li><code>11:20</code> Investigation: Architecture ~ Report Back (10 mins)</li> <li><code>11:30</code> Practice: Lab 2.2 ~ 05 Dataflows Gen2 (40 mins)</li> <li><code>12:10</code> Discussion: Debrief about the lab (10 mins)</li> </ul>"},{"location":"agenda/day2/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day2/#session-3","title":"Session 3","text":"<ul> <li><code>14:00</code> Practice: Lab 2.3 ~ 04 Ingest Pipeline (30 mins)</li> <li><code>14:30</code> Discussion: Debrief about the lab (10 mins)</li> </ul>"},{"location":"agenda/day2/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day2/#session-4","title":"Session 4","text":"<ul> <li><code>15:00</code> Investigation: ETL Product Investigation (30 mins)</li> <li><code>15:30</code> Report-Back: ETL Investigation ~ Report back (20 mins)</li> <li><code>15:50</code> WRAP (10 mins)</li> </ul>"},{"location":"agenda/day3/","title":"Day 3 - Microsoft Fabric","text":""},{"location":"agenda/day3/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome: Welcome to: DE5 Module 3 ~ Day 3 (10 mins)</li> </ul>"},{"location":"agenda/day3/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day3/#session-2","title":"Session 2","text":""},{"location":"agenda/day3/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day3/#session-3","title":"Session 3","text":""},{"location":"agenda/day3/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day3/#session-4","title":"Session 4","text":"<ul> <li><code>15:50</code> WRAP (10 mins)</li> </ul>"},{"location":"agenda/day4/","title":"Day 4 - ETL Patterns","text":""},{"location":"agenda/day4/#session-1","title":"Session 1","text":"<ul> <li><code>09:30</code> Welcome: Welcome to: DE5 Module 3 ~ Day 4 (10 mins)</li> </ul>"},{"location":"agenda/day4/#morning-break","title":"\u2615 Morning Break","text":""},{"location":"agenda/day4/#session-2","title":"Session 2","text":""},{"location":"agenda/day4/#lunch-break","title":"\ud83e\udd6a\ud83e\udd64 Lunch Break","text":""},{"location":"agenda/day4/#session-3","title":"Session 3","text":""},{"location":"agenda/day4/#afternoon-break","title":"\u2615 Afternoon Break","text":""},{"location":"agenda/day4/#session-4","title":"Session 4","text":"<ul> <li><code>15:50</code> WRAP (10 mins)</li> </ul>"},{"location":"day1/extract-lab/","title":"Morning Session 2: Extract Focus","text":""},{"location":"day1/extract-lab/#bridge-from-transform","title":"Bridge from Transform","text":"<p>\"You've cleaned internal data. Now let's enrich it with external sources - this is the Extract component.\"</p>"},{"location":"day1/extract-lab/#api-integration-lab","title":"API Integration Lab","text":""},{"location":"day1/extract-lab/#key-teaching-moments","title":"Key Teaching Moments","text":"<ul> <li>API failures are normal - show error handling</li> <li>Rate limiting matters - explain delays between calls</li> <li>Business logic applies - not just technical integration</li> </ul>"},{"location":"day1/extract-lab/#facilitation-strategy","title":"Facilitation Strategy","text":"<ul> <li>First 15 mins: Everyone does postcode API together</li> <li>Next 30 mins: Independent enrichment work</li> <li>Last 10 mins: Results comparison and discussion</li> </ul>"},{"location":"day1/extract-setup/","title":"Morning Session 2: Extract Focus","text":""},{"location":"day1/extract-setup/#bridge-from-transform","title":"Bridge from Transform","text":"<p>\"You've cleaned internal data. Now let's enrich it with external sources - this is the Extract component.\"</p>"},{"location":"day1/extract-setup/#api-integration-lab","title":"API Integration Lab","text":""},{"location":"day1/extract-setup/#key-teaching-moments","title":"Key Teaching Moments","text":"<ul> <li>API failures are normal - show error handling</li> <li>Rate limiting matters - explain delays between calls</li> <li>Business logic applies - not just technical integration</li> </ul>"},{"location":"day1/extract-setup/#facilitation-strategy","title":"Facilitation Strategy","text":"<ul> <li>First 15 mins: Everyone does postcode API together</li> <li>Next 30 mins: Independent enrichment work</li> <li>Last 10 mins: Results comparison and discussion</li> </ul>"},{"location":"day1/transform-lab/","title":"Session 1: Transform Lab","text":""},{"location":"day1/transform-lab/#lab-introduction","title":"Lab Introduction","text":"<ul> <li>Business scenario: CRM data quality project</li> <li>Success criteria: clean, validated, reportable data</li> <li>Quality metrics matter (not just completion)</li> </ul>"},{"location":"day1/transform-setup/","title":"Session 1: Transform Focus","text":""},{"location":"day1/transform-setup/#opening-hook","title":"Opening Hook","text":"<p>Show the messy customer data:</p> <p>\"This is what real data looks like. Your job: make it usable for business.\"</p>"},{"location":"day1/transform-setup/#lab-introduction","title":"Lab Introduction","text":"<ul> <li>Business scenario: CRM data quality project</li> <li>Success criteria: clean, validated, reportable data</li> <li>Quality metrics matter (not just completion)</li> </ul>"},{"location":"day1/transform-setup/#hands-on-lab","title":"Hands-on Lab","text":""},{"location":"day1/transform-setup/#l1-track-guided","title":"L1 Track (Guided)","text":"<ul> <li>Pair them up if possible</li> <li>Walk through first 2 examples together</li> <li>Check every 15 mins, provide hints</li> <li>Focus on concept understanding over syntax</li> </ul>"},{"location":"day1/transform-setup/#l2-track-semi-independent","title":"L2 Track (Semi-Independent)","text":"<ul> <li>Brief requirements overview</li> <li>Check every 20 mins</li> <li>Help with logic, not syntax</li> <li>Encourage experimentation</li> </ul>"},{"location":"day1/transform-setup/#l3-track-advanced","title":"L3 Track (Advanced)","text":"<ul> <li>Full requirements only</li> <li>Available for architecture discussions</li> <li>Challenge with edge cases</li> <li>May finish early (mentoring opportunity)</li> </ul>"},{"location":"day1/transform-setup/#wrap-up","title":"Wrap-up","text":"<p>Quick wins celebration:</p> <ul> <li>Show one data quality issue you fixed</li> <li>What business impact does clean data have?</li> </ul>"},{"location":"day1/transform-setup/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":"<ul> <li>pandas syntax errors: Point to examples, don't fix for them</li> <li>Logic confusion: Ask \"what should happen?\" before \"how to code it\"</li> <li>L1 learners stuck: Pair with L2 learner</li> <li>L3 learners finished early: Code review with others</li> </ul>"},{"location":"day4/medallion/group-scenarios/","title":"Group Scenarios","text":""},{"location":"day4/medallion/group-scenarios/#group-a-e-commerce-retailer","title":"Group A: E-commerce Retailer","text":"<ul> <li>Customer profiles + order management + returns processing</li> <li>Marketing segmentation, customer service efficiency, refund reporting</li> <li>Customer 360 view, product performance analytics</li> </ul>"},{"location":"day4/medallion/group-scenarios/#group-b-healthcare-clinic","title":"Group B: Healthcare Clinic","text":"<ul> <li>Patient management system + appointment booking + treatment records</li> <li>Privacy compliance (GDPR), appointment scheduling conflicts</li> <li>Clinical reporting, patient care coordination</li> </ul>"},{"location":"day4/medallion/group-scenarios/#group-c-regional-bank","title":"Group C: Regional Bank","text":"<ul> <li>Account management + transaction processing + compliance reporting</li> <li>Fraud detection, regulatory reporting, customer 360 view</li> <li>Real-time alerts vs batch reporting</li> </ul>"},{"location":"day4/medallion/group-scenarios/#group-d-manufacturing-plant","title":"Group D: Manufacturing Plant","text":"<ul> <li>IoT sensors + maintenance schedules + quality control</li> <li>Predictive maintenance, quality dashboards, compliance tracking</li> <li>High-frequency sensor data vs periodic inspections</li> </ul>"},{"location":"day4/medallion/report-back/","title":"Medallion Architecture: Group Presentations","text":""},{"location":"day4/medallion/report-back/#common-facilitation-questions-for-all-groups","title":"Common Facilitation Questions for All Groups","text":"<ul> <li>What's your biggest data quality challenge moving Bronze\u2192Silver?</li> <li>Who are your Gold layer users and what decisions do they make?</li> <li>How does your medallion design solve the business challenges listed?</li> </ul>"},{"location":"day4/medallion/report-back/#report-back-structure","title":"Report-back Structure","text":"<ul> <li>Describe your scenario</li> <li>Present your medallion design</li> <li>Class questions/feedback</li> </ul>"},{"location":"day4/medallion/report-back/#facilitation-tips","title":"Facilitation Tips","text":"<ul> <li>What's your biggest Bronze\u2192Silver cleaning challenge?</li> <li>How does your Gold layer serve different business users?</li> <li>What happens when source data structure changes?</li> </ul>"},{"location":"day4/medallion/report-back/#synthesis-bridge","title":"Synthesis &amp; Bridge","text":""},{"location":"day4/medallion/report-back/#key-pattern-recognition","title":"Key Pattern Recognition:","text":"<ul> <li>Notice: every industry has the same pattern - raw \u2192 trusted \u2192 business-ready</li> <li>The layers are consistent, but the business rules change</li> <li>This scales from your Day 1 customer pipeline to enterprise data lakes</li> </ul> <p>Medallion organises your data architecture. But how often do you rebuild each layer?</p> <p>Session 2: when to process everything vs just the changes.</p>"},{"location":"day4/medallion/scenario-brief-a/","title":"Group A: E-commerce","text":""},{"location":"day4/medallion/scenario-brief-a/#business-context","title":"Business Context","text":"<p>You work for \"TechStyle\" - an online retailer selling electronics and fashion. 50,000 orders/month, 200,000 active customers.</p>"},{"location":"day4/medallion/scenario-brief-a/#current-data-challenges","title":"Current Data Challenges","text":"<ul> <li>Customer service can't see complete order history</li> <li>Marketing sends emails to customers who've returned everything</li> <li>Finance struggles with refund reporting</li> <li>Website recommendations are based on stale data</li> </ul>"},{"location":"day4/medallion/scenario-brief-a/#your-data-sources","title":"Your Data Sources","text":""},{"location":"day4/medallion/scenario-brief-a/#website-database","title":"WEBSITE DATABASE","text":"<ul> <li>customer_profiles: ID, name, email, registration_date, preferences</li> <li>orders: order_id, customer_id, total_amount, order_date, status</li> <li>order_items: order_id, product_id, quantity, unit_price</li> </ul>"},{"location":"day4/medallion/scenario-brief-a/#returns-system-separate","title":"RETURNS SYSTEM (separate)","text":"<ul> <li>returns: return_id, order_id, reason, return_date, refund_status</li> <li>return_items: return_id, product_id, condition, refund_amount</li> </ul>"},{"location":"day4/medallion/scenario-brief-a/#external-apis","title":"EXTERNAL APIs","text":"<ul> <li>Payment gateway: transaction details, payment methods</li> <li>Shipping provider: delivery status, tracking</li> </ul>"},{"location":"day4/medallion/scenario-brief-a/#business-questions-to-answer","title":"Business Questions to Answer","text":"<ul> <li>Who are our most valuable customers? (Gold layer)</li> <li>Which products have high return rates? (Gold layer)</li> <li>Real-time order status for customer service (Silver layer)</li> <li>Daily sales dashboard for management (Gold layer)</li> </ul> <p>Use the worksheet to design Bronze \u2192 Silver \u2192 Gold data layers for TechStyle.</p>"},{"location":"day4/medallion/scenario-brief-a/#question-worksheet","title":"Question worksheet","text":"<pre><code>BRONZE Layer (Raw)\n\n- What data arrives? (format, frequency, quality)\n- Who owns this data?\n- Any compliance/privacy concerns?\n\nSILVER Layer (Cleaned)\n\n- What cleaning rules apply?\n- How do you handle quality issues?\n- What validation checks are needed?\n\nGOLD Layer (Business Ready)\n\n- What business questions does this answer?\n- Who are your end users?\n- What aggregations/metrics do you create?\n</code></pre>"},{"location":"day4/medallion/scenario-brief-a/#report-back","title":"Report back","text":"<ul> <li>Describe your scenario</li> <li>Present your medallion design</li> <li>Class questions/feedback</li> </ul>"},{"location":"day4/medallion/scenario-brief-b/","title":"Group B: Healthcare Clinic","text":""},{"location":"day4/medallion/scenario-brief-b/#business-context","title":"Business Context","text":"<p>You work for \"WellCare Medical Centre\" - a multi-specialty clinic with 15 doctors, 5 locations, serving 25,000 patients annually.</p>"},{"location":"day4/medallion/scenario-brief-b/#current-data-challenges","title":"Current Data Challenges","text":"<ul> <li>Doctors can't see patient history from other locations</li> <li>Appointment scheduling double-books specialists</li> <li>Insurance claims are manually processed and error-prone</li> <li>Patient care coordination between specialists is poor</li> <li>Compliance reporting takes weeks to compile</li> </ul>"},{"location":"day4/medallion/scenario-brief-b/#your-data-sources","title":"Your Data Sources","text":""},{"location":"day4/medallion/scenario-brief-b/#patient-management-system","title":"PATIENT MANAGEMENT SYSTEM","text":"<ul> <li>patients: patient_id, name, DOB, NHS_number, address, GP_practice</li> <li>appointments: appointment_id, patient_id, doctor_id, datetime, location, status</li> <li>consultations: consultation_id, appointment_id, diagnosis_codes, treatment_notes</li> </ul>"},{"location":"day4/medallion/scenario-brief-b/#clinical-records-separate-system","title":"CLINICAL RECORDS (separate system)","text":"<ul> <li>prescriptions: prescription_id, patient_id, medication, dosage, prescribed_date</li> <li>test_results: test_id, patient_id, test_type, results, lab_date</li> <li>referrals: referral_id, patient_id, from_doctor, to_specialist, reason</li> </ul>"},{"location":"day4/medallion/scenario-brief-b/#external-systems","title":"EXTERNAL SYSTEMS","text":"<ul> <li>NHS patient registry: demographics, GP details</li> <li>Insurance system: coverage details, claim status</li> <li>Pharmacy network: prescription fulfillment</li> </ul>"},{"location":"day4/medallion/scenario-brief-b/#business-questions-to-answer","title":"Business Questions to Answer","text":"<ul> <li>Which patients are overdue for follow-ups? (Gold layer)</li> <li>Doctor utilisation and patient flow optimisation (Gold layer)</li> <li>Real-time appointment availability (Silver layer)</li> <li>Compliance reporting for NHS contracts (Gold layer)</li> <li>Patient care pathway tracking (Gold layer)</li> </ul>"},{"location":"day4/medallion/scenario-brief-b/#compliance-considerations","title":"Compliance Considerations","text":"<ul> <li>GDPR patient privacy requirements</li> <li>NHS data sharing protocols</li> <li>Clinical governance standards</li> </ul> <p>Use the worksheet to design Bronze \u2192 Silver \u2192 Gold data layers for WellCare Medical Centre.</p>"},{"location":"day4/medallion/scenario-brief-b/#question-worksheet","title":"Question worksheet","text":"<pre><code>BRONZE Layer (Raw)\n\n- What data arrives? (format, frequency, quality)\n- Who owns this data?\n- Any compliance/privacy concerns?\n\nSILVER Layer (Cleaned)\n\n- What cleaning rules apply?\n- How do you handle quality issues?\n- What validation checks are needed?\n\nGOLD Layer (Business Ready)\n\n- What business questions does this answer?\n- Who are your end users?\n- What aggregations/metrics do you create?\n</code></pre>"},{"location":"day4/medallion/scenario-brief-b/#report-back","title":"Report back","text":"<ul> <li>Describe your scenario</li> <li>Present your medallion design</li> <li>Class questions/feedback</li> </ul>"},{"location":"day4/medallion/scenario-brief-c/","title":"Group 3: Regional Bank","text":""},{"location":"day4/medallion/scenario-brief-c/#business-context","title":"Business Context","text":"<p>You work for \"Yorkshire Community Bank\" - 50 branches, 150,000 customers, focusing on personal and small business banking.</p>"},{"location":"day4/medallion/scenario-brief-c/#current-data-challenges","title":"Current Data Challenges","text":"<ul> <li>Fraud alerts arrive too late to prevent losses</li> <li>Customer service can't see complete relationship (savings + loans + credit cards)</li> <li>Regulatory reports are manual and time-consuming</li> <li>Cross-selling opportunities are missed</li> <li>Risk assessment relies on outdated information</li> </ul>"},{"location":"day4/medallion/scenario-brief-c/#your-data-sources","title":"Your Data Sources","text":""},{"location":"day4/medallion/scenario-brief-c/#core-banking-system","title":"CORE BANKING SYSTEM:","text":"<ul> <li>customers: customer_id, name, address, risk_profile, relationship_manager</li> <li>accounts: account_id, customer_id, account_type, balance, opening_date</li> <li>transactions: transaction_id, account_id, amount, transaction_type, timestamp</li> </ul>"},{"location":"day4/medallion/scenario-brief-c/#lending-system-separate","title":"LENDING SYSTEM (separate):","text":"<ul> <li>loans: loan_id, customer_id, amount, interest_rate, term, status</li> <li>loan_payments: payment_id, loan_id, amount, due_date, paid_date</li> <li>credit_assessments: assessment_id, customer_id, credit_score, assessment_date</li> </ul>"},{"location":"day4/medallion/scenario-brief-c/#external-systems","title":"EXTERNAL SYSTEMS:","text":"<ul> <li>Credit reference agencies: credit scores, payment history</li> <li>Open banking APIs: external account data (with consent)</li> <li>Fraud detection service: risk scores, alert flags</li> </ul>"},{"location":"day4/medallion/scenario-brief-c/#business-questions-to-answer","title":"Business Questions to Answer","text":"<ul> <li>Who are high-value customers eligible for premium services? (Gold layer)</li> <li>Real-time fraud risk assessment (Silver layer)</li> <li>Regulatory capital adequacy reporting (Gold layer)</li> <li>Customer lifetime value and churn prediction (Gold layer)</li> <li>Branch performance and profitability analysis (Gold layer)</li> </ul>"},{"location":"day4/medallion/scenario-brief-c/#compliance-considerations","title":"Compliance Considerations","text":"<ul> <li>FCA regulatory reporting requirements</li> <li>PCI DSS payment card security</li> <li>Open banking consent management</li> <li>Anti-money laundering (AML) monitoring</li> </ul> <p>Use the worksheet to design Bronze \u2192 Silver \u2192 Gold data layers for Yorkshire Community Bank.</p>"},{"location":"day4/medallion/scenario-brief-c/#question-worksheet","title":"Question worksheet","text":"<pre><code>BRONZE Layer (Raw)\n\n- What data arrives? (format, frequency, quality)\n- Who owns this data?\n- Any compliance/privacy concerns?\n\nSILVER Layer (Cleaned)\n\n- What cleaning rules apply?\n- How do you handle quality issues?\n- What validation checks are needed?\n\nGOLD Layer (Business Ready)\n\n- What business questions does this answer?\n- Who are your end users?\n- What aggregations/metrics do you create?\n</code></pre>"},{"location":"day4/medallion/scenario-brief-c/#report-back","title":"Report back","text":"<ul> <li>Describe your scenario</li> <li>Present your medallion design</li> <li>Class questions/feedback</li> </ul>"},{"location":"day4/medallion/scenario-brief-d/","title":"Group 4: Manufacturing Plant","text":""},{"location":"day4/medallion/scenario-brief-d/#business-context","title":"Business Context","text":"<p>You work for \"Northern Steel Manufacturing\" - automotive parts production, 3 production lines, 24/7 operations, 2 million parts annually.</p>"},{"location":"day4/medallion/scenario-brief-d/#current-data-challenges","title":"Current Data Challenges","text":"<ul> <li>Equipment breakdowns cause expensive production delays</li> <li>Quality issues discovered too late in the process</li> <li>Energy costs are high but poorly understood</li> <li>Maintenance is reactive rather than predictive</li> <li>Production planning relies on outdated data</li> </ul>"},{"location":"day4/medallion/scenario-brief-d/#your-data-sources","title":"Your Data Sources","text":""},{"location":"day4/medallion/scenario-brief-d/#iot-sensor-network","title":"IoT SENSOR NETWORK","text":"<ul> <li>machine_sensors: sensor_id, machine_id, temperature, pressure, vibration, timestamp</li> <li>production_line: line_id, production_rate, downtime_minutes, shift_datetime</li> <li>quality_sensors: sensor_id, part_id, dimensions, weight, defect_flags</li> </ul>"},{"location":"day4/medallion/scenario-brief-d/#maintenance-system-separate","title":"MAINTENANCE SYSTEM (separate)","text":"<ul> <li>work_orders: order_id, machine_id, issue_type, scheduled_date, completion_date</li> <li>spare_parts: part_id, machine_id, stock_level, cost, supplier</li> <li>maintenance_history: history_id, machine_id, maintenance_type, technician, notes</li> </ul>"},{"location":"day4/medallion/scenario-brief-d/#external-systems","title":"EXTERNAL SYSTEMS","text":"<ul> <li>Energy supplier: usage data, tariff rates, demand charges</li> <li>Supplier systems: raw material delivery schedules, quality certificates</li> <li>Customer orders: order_id, part_specifications, delivery_requirements</li> </ul>"},{"location":"day4/medallion/scenario-brief-d/#business-questions-to-answer","title":"Business Questions to Answer:","text":"<ul> <li>Which machines need predictive maintenance? (Gold layer)</li> <li>Real-time production line efficiency (Silver layer)</li> <li>Quality control trending and root cause analysis (Gold layer)</li> <li>Energy consumption optimisation (Gold layer)</li> <li>Overall Equipment Effectiveness (OEE) reporting (Gold layer)</li> </ul>"},{"location":"day4/medallion/scenario-brief-d/#operational-considerations","title":"Operational Considerations:","text":"<ul> <li>High-frequency data (sensors every 30 seconds)</li> <li>24/7 operations - no maintenance windows</li> <li>Safety-critical systems requiring immediate alerts</li> <li>Integration with production planning systems</li> </ul> <p>Use the worksheet to design Bronze \u2192 Silver \u2192 Gold data layers for Northern Steel Manufacturing.</p>"},{"location":"day4/medallion/scenario-brief-d/#question-worksheet","title":"Question worksheet","text":"<pre><code>BRONZE Layer (Raw)\n\n- What data arrives? (format, frequency, quality)\n- Who owns this data?\n- Any compliance/privacy concerns?\n\nSILVER Layer (Cleaned)\n\n- What cleaning rules apply?\n- How do you handle quality issues?\n- What validation checks are needed?\n\nGOLD Layer (Business Ready)\n\n- What business questions does this answer?\n- Who are your end users?\n- What aggregations/metrics do you create?\n</code></pre>"},{"location":"day4/medallion/scenario-brief-d/#report-back","title":"Report back","text":"<ul> <li>Describe your scenario</li> <li>Present your medallion design</li> <li>Class questions/feedback</li> </ul>"},{"location":"labs/01-lakehouse/","title":"Lab ~ Create a Microsoft Fabric Lakehouse","text":"<p>Large-scale data analytics solutions have traditionally been built around a data warehouse, in which data is stored in relational tables and queried using SQL. The growth in \u201cbig data\u201d (characterized by high volumes, variety, and velocity of new data assets) together with the availability of low-cost storage and cloud-scale distributed compute technologies has led to an alternative approach to analytical data storage; the data lake. In a data lake, data is stored as files without imposing a fixed schema for storage. Increasingly, data engineers and analysts seek to benefit from the best features of both of these approaches by combining them in a data lakehouse; in which data is stored in files in a data lake and a relational schema is applied to them as a metadata layer so that they can be queried using traditional SQL semantics.</p> <p>In Microsoft Fabric, a lakehouse provides highly scalable file storage in a OneLake store (built on Azure Data Lake Store Gen2) with a metastore for relational objects such as tables and views based on the open source Delta Lake table format. Delta Lake enables you to define a schema of tables in your lakehouse that you can query using SQL.</p> <p>This lab takes approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p>"},{"location":"labs/01-lakehouse/#step-1-signing-in-to-microsoft-fabric","title":"Step 1: Signing in to Microsoft Fabric","text":"<p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"labs/01-lakehouse/#step-2-create-a-workspace","title":"Step 2: Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"labs/01-lakehouse/#step-3-create-a-lakehouse","title":"Step 3: Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> <li> <p>View the new lakehouse, and note that the Lakehouse explorer pane on the left enables you to browse tables and files in the lakehouse:</p> <ul> <li> <p>The Tables folder contains tables that you can query using SQL semantics. Tables in a Microsoft Fabric lakehouse are based on the open source Delta Lake file format, commonly used in Apache Spark.</p> </li> <li> <p>The Files folder contains data files in the OneLake storage for the lakehouse that aren't associated with managed delta tables. You can also create shortcuts in this folder to reference data that is stored externally.</p> </li> </ul> </li> </ol> <p>Currently, there are no tables or files in this lakehouse.</p>"},{"location":"labs/01-lakehouse/#step-4-upload-a-file","title":"Step 4: Upload a file","text":"<p>Fabric provides multiple ways to load data into the lakehouse, including built-in support for pipelines that copy data from external sources and data flows (Gen 2) that you can define using visual tools based on Power Query. However one of the simplest ways to ingest small amounts of data is to upload files or folders from your local computer (or lab VM if applicable).</p> <ol> <li> <p>Download the sales.csv file from https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv, </p> <ul> <li>Save it as <code>sales.csv</code> on your local computer (or lab VM if applicable).</li> </ul> <p>Note</p> <ul> <li>To download the file, open a new tab in the browser and paste in the URL.</li> <li>Right click anywhere on the page containing the data and select \"Save as\" to save the data as a CSV file.</li> </ul> </li> <li> <p>Return to the web browser tab containing your lakehouse</p> <ul> <li>Click the ... menu for the Files folder in the Explorer pane select New subfolder</li> <li>Name the new subfolder: <code>data</code></li> <li>Click Create</li> </ul> </li> <li> <p>In the ... menu for the new data folder, select Upload and Upload files.</p> <ul> <li>Then upload the sales.csv file from your local computer (or lab VM if applicable).</li> </ul> </li> <li> <p>After the file has been uploaded, select the Files/data folder and verify that the sales.csv file has been uploaded, as shown here:</p> <p></p> </li> <li> <p>Select the sales.csv file to see a preview of its contents.</p> <p>If the sales.csv file does not automatically appear, in the ... menu for the data folder, select Refresh.</p> </li> </ol>"},{"location":"labs/01-lakehouse/#step-5-explore-shortcuts","title":"Step 5: Explore shortcuts","text":"<p>In many scenarios, the data you need to work with in your lakehouse may be stored in some other location. While there are many ways to ingest data into the OneLake storage for your lakehouse, another option is to instead create a shortcut. Shortcuts enable you to include externally sourced data in your analytics solution without the overhead and risk of data inconsistency associated with copying it.</p> <ol> <li> <p>In the ... menu for the Files folder, select New shortcut.</p> </li> <li> <p>View the available data source types for shortcuts.</p> <ul> <li>Then close the New shortcut dialog box without creating a shortcut.</li> </ul> </li> </ol>"},{"location":"labs/01-lakehouse/#step-6-load-file-data-into-a-table","title":"Step 6: Load file data into a table","text":"<p>The sales data you uploaded is in a file, which you can work with directly by using Apache Spark code. However, in many scenarios you may want to load the data from the file into a table so that you can query it using SQL.</p> <ol> <li> <p>In the Explorer pane, select the Files/data folder so you can see the sales.csv file it contains.</p> </li> <li> <p>In the ... menu for the sales.csv file, select Load to Tables &gt; New table.</p> <p></p> </li> <li> <p>In Load to table dialog box, set the table name to sales and confirm the load operation.</p> <ul> <li>Then wait for the table to be created and loaded.</li> </ul> <p>If the sales table does not automatically appear, in the ... menu for the Tables folder, select Refresh.</p> </li> <li> <p>In the Explorer pane, select the sales table that has been created to view the data:</p> <p></p> </li> <li> <p>In the ... menu for the sales table, select View files to see the underlying files for this table:</p> <p></p> <p>Files for a delta table are stored in Parquet format, and include a subfolder named <code>_delta_log</code> in which details of transactions applied to the table are logged.</p> </li> </ol>"},{"location":"labs/01-lakehouse/#step-7-use-sql-to-query-tables","title":"Step 7: Use SQL to query tables","text":"<p>When you create a lakehouse and define tables in it, a SQL endpoint is automatically created through which the tables can be queried using SQL <code>SELECT</code> statements.</p> <ol> <li> <p>At the top-right of the Lakehouse page, switch from Lakehouse to SQL analytics endpoint.</p> <ul> <li>Then wait a short time until the SQL analytics endpoint for your lakehouse opens in a visual interface from which you can query its tables.</li> </ul> </li> <li> <p>Use the New SQL query button to open a new query editor, and enter the following SQL query:</p> <pre><code>SELECT Item, SUM(Quantity * UnitPrice) AS Revenue\nFROM sales\nGROUP BY Item\nORDER BY Revenue DESC;\n</code></pre> </li> <li> <p>Use the  Run button to run the query and view the results, which should show the total revenue for each product.</p> <p></p> </li> </ol>"},{"location":"labs/01-lakehouse/#step-8-create-a-visual-query","title":"Step 8: Create a visual query","text":"<p>While many data professionals are familiar with SQL, those with Power BI experience can apply their Power Query skills to create visual queries.</p> <ol> <li> <p>On the toolbar, expand the New SQL query option and select New visual query.</p> </li> <li> <p>Drag the sales table (under dbo &gt; Tables) to the new visual query editor pane that opens to create a Power Query as shown here:</p> <p></p> </li> <li> <p>In the Manage columns menu, select Choose columns.</p> <ul> <li>Then select only the SalesOrderNumber and SalesOrderLineNumber columns. Click OK</li> </ul> <p></p> </li> <li> <p>in the Transform menu, select Group by. Then group the data by using the following Basic settings:</p> <ul> <li>Group by: SalesOrderNumber</li> <li>New column name: <code>LineItems</code></li> <li>Operation: Count distinct values</li> <li>Column: SalesOrderLineNumber (if not greyed out)</li> </ul> <p>When you're done, the results pane under the visual query shows the number of line items for each sales order.</p> <p></p> </li> </ol>"},{"location":"labs/01-lakehouse/#clean-up-resources","title":"Clean up resources","text":"<p>In this exercise, you have created a lakehouse and imported data into it. You\u2019ve seen how a lakehouse consists of files and tables stored in a OneLake data store. The managed tables can be queried using SQL, and are included in a default semantic model to support data visualizations.</p> <p>If you've finished exploring your lakehouse, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/01-lakehouse.html </p>"},{"location":"labs/04-ingest-pipeline/","title":"Lab: Ingest Data with a Pipeline in Microsoft Fabric","text":"<p>A data lakehouse is a common analytical data store for cloud-scale analytics solutions. One of the core tasks of a data engineer is to implement and manage the ingestion of data from multiple operational data sources into the lakehouse. In Microsoft Fabric, you can implement extract, transform, and load (ETL) or extract, load, and transform (ELT) solutions for data ingestion through the creation of pipelines.</p> <p>Fabric also supports Apache Spark, enabling you to write and run code to process data at scale. By combining the pipeline and Spark capabilities in Fabric, you can implement complex data ingestion logic that copies data from external sources into the OneLake storage on which the lakehouse is based, and then uses Spark code to perform custom data transformations before loading it into tables for analysis.</p> <p>This lab will take approximately 45 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p> <p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> </ol> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p>"},{"location":"labs/04-ingest-pipeline/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a uniquename of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new lakehouse with no Tables or Files will be created.</p> <p></p> </li> <li> <p>On the Explorer pane on the left, in the ... menu for the Files node, select New subfolder and create a subfolder named new_data</p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#create-a-pipeline","title":"Create a pipeline","text":"<p>A simple way to ingest data is to use a Copy Data activity in a pipeline to extract the data from a source and copy it to a file in the lakehouse.</p> <ol> <li> <p>On the Home page for your lakehouse, select Get data and then select New data pipeline, and create a new data pipeline named <code>Ingest Sales Data</code></p> </li> <li> <p>If the Copy Data wizard doesn't open automatically, select Copy Data &gt; Use copy assistant in the pipeline editor page.</p> </li> <li> <p>In the Copy Data wizard, on the Choose data source page, type HTTP in the search bar and then select HTTP in the New sources section.</p> <p></p> </li> <li> <p>In the Connect to data source pane, enter the following settings for the connection to your data source:</p> <ul> <li>URL: https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/sales.csv</li> <li>Connection: Create new connection</li> <li>Connection name: Specify a unique name</li> <li>Data gateway: (none)</li> <li>Authentication kind: Anonymous</li> </ul> </li> <li> <p>Select Next. Then ensure the following settings are selected:</p> <ul> <li>Relative URL: Leave blank</li> <li>Request method: GET</li> <li>Additional headers: Leave blank</li> <li>Binary copy: Unselected</li> <li>Request timeout: Leave blank</li> <li>Max concurrent connections: Leave blank</li> </ul> </li> <li> <p>Select Next, and wait for the data to be sampled and then ensure that the following settings are selected:</p> <ul> <li>File format: DelimitedText</li> <li>Column delimiter: Comma (,)</li> <li>Row delimiter: Line feed (\\n)</li> <li>First row as header: Selected</li> <li>Compression type: None</li> </ul> </li> <li> <p>Select Preview data to see a sample of the data that will be ingested. Then close the data preview and select Next.</p> </li> <li> <p>On the Connect to data destination page, set the following data destination options, and then select Next:</p> <ul> <li>Root folder: Files</li> <li>Folder path name: new_data</li> <li>File name: sales.csv</li> <li>Copy behavior: None</li> </ul> </li> <li> <p>Set the following file format options and then select Next:</p> <ul> <li>File format: DelimitedText</li> <li>Column delimiter: Comma (,)</li> <li>Row delimiter: Line feed (\\n)</li> <li>Add header to file: Selected</li> <li>Compression type: None</li> </ul> </li> <li> <p>On the Copy summary page, review the details of your copy operation and then select Save + Run.</p> <p>A new pipeline containing a Copy Data activity is created, as shown here:</p> <p></p> </li> <li> <p>When the pipeline starts to run, you can monitor its status in the Output pane under the pipeline designer. Use the  (Refresh) icon to refresh the status, and wait until it has succeeeded.</p> </li> <li> <p>In the menu bar on the left, select your lakehouse.</p> </li> <li> <p>On the Home page, in the Explorer pane, expand Files and select the new_data folder to verify that the sales.csv file has been copied.</p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#create-a-notebook","title":"Create a notebook","text":"<ol> <li> <p>On the Home page for your lakehouse, in the Open notebook menu, select New notebook.</p> <p>After a few seconds, a new notebook containing a single cell will open. Notebooks are made up of one or more cells that can contain code or markdown (formatted text).</p> </li> <li> <p>Select the existing cell in the notebook, which contains some simple code, and then replace the default code with the following variable declaration.</p> <pre><code>table_name = \"sales\"\n</code></pre> </li> <li> <p>In the ... menu for the cell (at its top-right) select Toggle parameter cell. This configures the cell so that the variables declared in it are treated as parameters when running the notebook from a pipeline.</p> </li> <li> <p>Under the parameters cell, use the + Code button to add a new code cell. Then add the following code to it:</p> <pre><code>from pyspark.sql.functions import *\n\n# Read the new sales data\ndf = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/new_data/*.csv\")\n\n## Add month and year columns\ndf = df.withColumn(\"Year\", year(col(\"OrderDate\"))).withColumn(\"Month\", month(col(\"OrderDate\")))\n\n# Derive FirstName and LastName columns\ndf = df.withColumn(\"FirstName\", split(col(\"CustomerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"CustomerName\"), \" \").getItem(1))\n\n# Filter and reorder columns\ndf = df[\"SalesOrderNumber\", \"SalesOrderLineNumber\", \"OrderDate\", \"Year\", \"Month\", \"FirstName\", \"LastName\", \"EmailAddress\", \"Item\", \"Quantity\", \"UnitPrice\", \"TaxAmount\"]\n\n# Load the data into a table\ndf.write.format(\"delta\").mode(\"append\").saveAsTable(table_name)\n</code></pre> <p>This code loads the data from the sales.csv file that was ingested by the Copy Data activity, applies some transformation logic, and saves the transformed data as a table - appending the data if the table already exists.</p> </li> <li> <p>Verify that your notebooks looks similar to this, and then use the  Run all button on the toolbar to run all of the cells it contains.</p> <p></p> <p>Note</p> <ul> <li>Since this is the first time you've run any Spark code in this session, the Spark pool must be started.</li> <li>This means that the first cell can take a minute or so to complete.</li> </ul> </li> <li> <p>When the notebook run has completed, in the Explorer pane on the left, in the ... menu for Tables select Refresh and verify that a sales table has been created.</p> </li> <li> <p>In the notebook menu bar, use the \u2699\ufe0f Settings icon to view the notebook settings. Then set the Name of the notebook to <code>Load Sales</code> and close the settings pane.</p> </li> <li> <p>In the hub menu bar on the left, select your lakehouse.</p> </li> <li> <p>In the Explorer pane, refresh the view. Then expand Tables, and select the sales table to see a preview of the data it contains.</p> </li> </ol>"},{"location":"labs/04-ingest-pipeline/#modify-the-pipeline","title":"Modify the pipeline","text":"<p>Now that you've implemented a notebook to transform data and load it into a table, you can incorporate the notebook into a pipeline to create a reusable ETL process.</p> <ol> <li> <p>In the hub menu bar on the left select the Ingest Sales Data pipeline you created previously.</p> </li> <li> <p>On the Activities tab, in the All activities list, select Delete data. Then position the new Delete data activity to the left of the Copy data activity and connect its On completion output to the Copy data activity, as shown here:</p> <p></p> </li> <li> <p>Select the Delete data activity, and in the pane below the design canvas, set the following properties:</p> <ul> <li> <p>General:</p> <ul> <li>Name: <code>Delete old files</code></li> </ul> </li> <li> <p>Source:</p> <ul> <li>Connection: Your lakehouse</li> <li>File path type: Wildcard file path</li> <li>Folder path: Files / new_data</li> <li>Wildcard file name: <code>*.csv</code></li> <li>Recursively: Selected</li> </ul> </li> <li> <p>Logging settings:</p> <ul> <li>Enable logging: Unselected</li> </ul> </li> </ul> <p>These settings will ensure that any existing .csv files are deleted before copying the sales.csv file.</p> </li> <li> <p>In the pipeline designer, on the Activities tab, select Notebook to add a Notebook activity to the pipeline.</p> </li> <li> <p>Select the Copy data activity and then connect its On Completion output to the Notebook activity as shown here:</p> <p></p> </li> <li> <p>Select the Notebook activity, and then in the pane below the design canvas, set the following properties:</p> <ul> <li> <p>General:</p> <ul> <li>Name: <code>Load Sales notebook</code></li> </ul> </li> <li> <p>Settings:</p> <ul> <li>Notebook: Load Sales</li> <li>Base parameters: Add a new parameter with the following properties:</li> </ul> Name Type Value table_name String new_sales </li> </ul> <p>The table_name parameter will be passed to the notebook and override the default value assigned to the table_name variable in the parameters cell.</p> </li> <li> <p>On the Home tab, use the  (Save) icon to save the pipeline. Then use the  Run button to run the pipeline, and wait for all of the activities to complete.</p> <p></p> <p>If you see an error message</p> <ul> <li>In case you receive the error message:<ul> <li>Spark SQL queries are only possible in the context of a lakehouse. Please attach a lakehouse to proceed:</li> </ul> </li> <li>Open your notebook, select the lakehouse you created on the left pane,</li> <li>select Remove all Lakehouses and then add it again.</li> <li>Go back to the pipeline designer and select  Run.</li> </ul> </li> <li> <p>In the hub menu bar on the left edge of the portal, select your lakehouse.</p> </li> <li> <p>In the Explorer pane, expand Tables and select the new_sales table to see a preview of the data it contains. This table was created by the notebook when it was run by the pipeline.</p> </li> </ol> <p>In this exercise, you implemented a data ingestion solution that uses a pipeline to copy data to your lakehouse from an external source, and then uses a Spark notebook to transform the data and load it into a table.</p>"},{"location":"labs/04-ingest-pipeline/#clean-up-resources","title":"Clean up resources","text":"<p>If you've finished exploring your lakehouse, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/04-ingest-pipeline.html </p>"},{"location":"labs/05-dataflows-gen2/","title":"Lab: Create and use Dataflows (Gen2) in Microsoft Fabric","text":"<p>In Microsoft Fabric, Dataflows (Gen2) connect to various data sources and perform transformations in Power Query Online. They can then be used in Data Pipelines to ingest data into a lakehouse or other analytical store, or to define a dataset for a Power BI report.</p> <p>This lab is designed to introduce the different elements of Dataflows (Gen2), and not create a complex solution that may exist in an enterprise. This lab takes approximately 30 minutes to complete.</p> <p>For this lab you need to navigate to QA Platform and login using the credentials provided</p> <p>It is important that you use an incognito/private mode browser tab and not your work or personal Microsoft login</p> <p>In this lab, you will sign in to Microsoft Fabric using the email and password from the QA Platform.</p> <ol> <li> <p>Using an incognito/private mode browser tab navigate to the Fabric portal at: https://fabric.microsoft.com</p> </li> <li> <p>Follow the prompts, and sign in with the user credentials from the QA Platform:</p> <ul> <li>Email</li> <li>Password</li> </ul> </li> <li> <p>After signing in, you will be redirected to the Fabric home page:</p> <p></p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#create-a-workspace","title":"Create a workspace","text":"<p>Before working with data in Fabric, you need to create a workspace with the Fabric trial enabled.</p> <ol> <li> <p>Navigate to the Microsoft Fabric home page in an incognito/private mode browser tab browser, and sign in with the Fabric credentials from the QA Platform.</p> </li> <li> <p>In the menu bar on the left, select Workspaces (the icon looks similar to \ud83d\uddc7).</p> </li> <li> <p>Create a New workspace:</p> <ul> <li>Give it a name of your choice. For example: <code>fab_workspace</code></li> <li>Leave all other options as the default values</li> <li>Click Apply</li> </ul> </li> <li> <p>When your new workspace opens, it should be empty.</p> <p></p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#create-a-lakehouse","title":"Create a lakehouse","text":"<p>Now that you have a workspace, it's time to create a data lakehouse into which you'll ingest data.</p> <ol> <li> <p>On the menu bar on the left, select Create. In the New page, under the Data Engineering section, select Lakehouse.</p> <ul> <li>Give it a name of your choice. For example: <code>fab_lakehouse</code></li> </ul> <p>If the Create option is not pinned to the sidebar, you need to select the ellipsis (\u2026) option first.</p> <p>After a minute or so, a new empty lakehouse will be created.</p> <p></p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#create-a-dataflow-gen2-to-ingest-data","title":"Create a Dataflow (Gen2) to ingest data","text":"<p>Now that you have a lakehouse, you need to ingest some data into it. One way to do this is to define a dataflow that encapsulates an extract, transform, and load (ETL) process.</p> <ol> <li> <p>In the home page for your lakehouse, select Get data &gt; New Dataflow Gen2</p> <p></p> <p>Click Create, and after a few seconds, the Power Query editor for your new dataflow opens as shown here:</p> <p></p> </li> <li> <p>Select Import from a Text/CSV file, and create a new data source with the following settings:</p> <ul> <li>Link to file: Selected</li> <li>File path or URL: https://raw.githubusercontent.com/MicrosoftLearning/dp-data/main/orders.csv</li> <li>Connection: Create new connection</li> <li>Connection Name: default value ~ or orders.csv if name already exists</li> <li>data gateway: (none)</li> <li>Authentication kind: Anonymous</li> <li>Privacy Level: None</li> </ul> </li> <li> <p>Select Next to preview the file data, and then Create the data source.</p> <p>The Power Query editor shows the data source and an initial set of query steps to format the data, as shown here:</p> <p></p> </li> <li> <p>On the toolbar ribbon, select the Add column tab. Then select Custom column and create a new column.</p> <p></p> </li> <li> <p>Do the following:</p> <ul> <li>Set the New column name to: MonthNo</li> <li>Set the Data type to: Whole number</li> <li>Add this Custom column formula: <code>Date.Month([OrderDate])</code></li> </ul> <p></p> </li> <li> <p>Click OK to create the column. Notice how the step to add the custom column is added to the query.</p> <p>The resulting column is displayed in the data pane:</p> <p></p> <p>Info</p> <ul> <li>In the Query Settings pane on the right side, notice the Applied Steps include each transformation step.</li> <li>At the bottom, you can also toggle the Diagram view button to turn on the Visual Diagram of the steps.</li> </ul> <p>Info</p> <p>Steps can be moved up or down, edited by selecting the gear icon, and you can select each step to see the transformations apply in the preview pane.</p> </li> <li> <p>Check and confirm that the data type for the OrderDate column is set to Date and the data type for the newly created column MonthNo is set to Whole Number.</p> <p></p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#add-data-destination-for-dataflow","title":"Add data destination for Dataflow","text":"<ol> <li> <p>On the toolbar ribbon, select the Home tab. Then in the Add data destination drop-down menu, select Lakehouse.</p> <p>Note</p> <ul> <li>If this option is grayed out, you may already have a data destination set.</li> <li>Check the data destination at the bottom of the Query settings pane on the right side of the Power Query editor.</li> <li>If a default destination is already set, you can remove it and add a new one.</li> </ul> </li> <li> <p>In the Connect to data destination dialog box, use the existing connection credentials:</p> <p></p> </li> <li> <p>Select Next and in the list of available workspaces, find your workspace and select the lakehouse you created in it at the start of this exercise. Then specify a new table named orders:</p> <p></p> </li> <li> <p>Select Next and on the Choose destination settings page:</p> <ul> <li>Disable the Use automatic settings option, select Append, and then Save settings.</li> </ul> <p></p> </li> <li> <p>On the Menu bar, open View and select Diagram view. Notice the Lakehouse destination is indicated as an icon in the query in the Power Query editor.</p> <p></p> </li> <li> <p>On the toolbar ribbon, select the Home tab. Then select Save &amp; run and wait for the Dataflow 1 dataflow to be created in your workspace.</p> </li> </ol>"},{"location":"labs/05-dataflows-gen2/#add-a-dataflow-to-a-pipeline","title":"Add a dataflow to a pipeline","text":"<p>You can include a dataflow as an activity in a pipeline. Pipelines are used to orchestrate data ingestion and processing activities, enabling you to combine dataflows with other kinds of operation in a single, scheduled process. Pipelines can be created in a few different experiences, including Data Factory experience.</p> <ol> <li> <p>From your Fabric-enabled workspace, select + New item &gt; Data pipeline</p> <ul> <li>When prompted, create a new pipeline named: Load data</li> </ul> <p>Click Create, and the pipeline editor will open:</p> <p></p> <p>If the Copy Data wizard opens automatically, you can just close it.</p> </li> <li> <p>Select Pipeline activity, and add a Dataflow activity to the pipeline.</p> </li> <li> <p>With the new Dataflow1 activity selected, on the Settings tab, in the Dataflow drop-down list, select Dataflow 1 (the data flow you created previously)</p> <p></p> </li> <li> <p>On the Home tab, save the pipeline using the  (Save) icon.</p> </li> <li> <p>Use the  Run button to run the pipeline, and wait for it to complete. It may take a few minutes.</p> <p></p> </li> <li> <p>In the menu bar on the left edge, select your lakehouse.</p> </li> <li> <p>In the ... menu for Tables, select refresh. </p> <p>Then expand Tables and select the orders table, which has been created by your dataflow.</p> <p></p> </li> </ol> Tip for Power Bi Desktop users: <ul> <li>In Power BI Desktop, you can connect directly to the data transformations done with your dataflow by using the Power BI dataflows (Legacy) connector.</li> <li>You can also make additional transformations, publish as a new dataset, and distribute with intended audience for specialized datasets.</li> </ul> <p></p>"},{"location":"labs/05-dataflows-gen2/#clean-up-resources","title":"Clean up resources","text":"<p>If you've finished exploring dataflows in Microsoft Fabric, you can delete the workspace you created for this exercise.</p> <ol> <li> <p>Navigate to Microsoft Fabric in your browser.</p> </li> <li> <p>In the bar on the left, select the icon for your workspace to view all of the items it contains.</p> </li> <li> <p>Select Workspace settings and in the General section, scroll down and select Remove this workspace.</p> </li> <li> <p>Select Delete to delete the workspace.</p> </li> </ol> <p>Source: https://microsoftlearning.github.io/mslearn-fabric/Instructions/Labs/05-dataflows-gen2.html </p>"}]}